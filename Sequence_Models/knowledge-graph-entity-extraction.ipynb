{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install BeautifulSoup4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport bs4\nimport requests\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom spacy.matcher import Matcher \nfrom spacy.tokens import Span \n\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\npd.set_option('display.max_colwidth', 200)\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences = pd.read_csv(\"../input/wiki-sentences/wiki_sentences_v2.csv\")\ncandidate_sentences.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['sentence'].sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc = nlp(\"the drawdown process is governed by astm standard d823\")\n\nfor tok in doc:\n  print(tok.text, \"...\", tok.dep_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_entities(sent):\n  ## chunk 1\n  ent1 = \"\"\n  ent2 = \"\"\n\n  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n  prv_tok_text = \"\"   # previous token in the sentence\n\n  prefix = \"\"\n  modifier = \"\"\n\n  #############################################################\n  \n  for tok in nlp(sent):\n    ## chunk 2\n    # if token is a punctuation mark then move on to the next token\n    if tok.dep_ != \"punct\":\n      # check: token is a compound word or not\n      if tok.dep_ == \"compound\":\n        prefix = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          prefix = prv_tok_text + \" \"+ tok.text\n      \n      # check: token is a modifier or not\n      if tok.dep_.endswith(\"mod\") == True:\n        modifier = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          modifier = prv_tok_text + \" \"+ tok.text\n      \n      ## chunk 3\n      if tok.dep_.find(\"subj\") == True:\n        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n        prefix = \"\"\n        modifier = \"\"\n        prv_tok_dep = \"\"\n        prv_tok_text = \"\"      \n\n      ## chunk 4\n      if tok.dep_.find(\"obj\") == True:\n        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n        \n      ## chunk 5  \n      # update variables\n      prv_tok_dep = tok.dep_\n      prv_tok_text = tok.text\n  #############################################################\n\n  return [ent1.strip(), ent2.strip()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_entities(\"the film had 200 patents\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"entity_pairs = []\n\nfor i in tqdm(candidate_sentences[\"sentence\"]):\n  entity_pairs.append(get_entities(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"entity_pairs[10:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_relation(sent):\n\n  doc = nlp(sent)\n\n#   for tok in doc:\n#     print(tok.text, \" \", tok.dep_, \" \", tok.pos_)\n\n  # Matcher class object \n  matcher = Matcher(nlp.vocab)\n\n  #define the pattern \n  pattern = [{'DEP':'ROOT'}, \n            {'DEP':'prep','OP':\"?\"},\n            {'DEP':'agent','OP':\"?\"},  \n            {'POS':'ADJ','OP':\"?\"}] \n\n  matcher.add(\"matching_1\", None, pattern) \n\n  matches = matcher(doc)\n#   print(\"Matches:\", matches)\n  k = len(matches) - 1\n#   print(\"K:\", k)\n  span = doc[matches[k][1]:matches[k][2]] \n\n  return(span.text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_relation(\"John completed the task\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_relation(\"John and Jan\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relations = [get_relation(i) for i in tqdm(candidate_sentences['sentence'])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series(relations).value_counts()[:50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract subject\nsource = [i[0] for i in entity_pairs]\n\n# extract object\ntarget = [i[1] for i in entity_pairs]\n\nkg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kg_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,12))\n\npos = nx.spring_layout(G)\nnx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"composed by\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"released in\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5)\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-pretrained-bert pytorch-nlp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Libraries\n\nimport tensorflow as tf\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertConfig\nfrom pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport io\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/cola-the-corpus-of-linguistic-acceptability/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create sentence and label lists\nsentences = df.sentence.values\n\n# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\nsentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\nlabels = df.label.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\nprint (\"Tokenize the first sentence:\")\nprint (tokenized_texts[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(input_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_ids[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(input_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_ids[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create attention masks\nattention_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n  seq_mask = [float(i>0) for i in seq]\n  attention_masks.append(seq_mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# attention_masks[:1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.label_notes.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use train_test_split to split our data into train and validation sets for training\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n                                                            random_state=2018, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                             random_state=2018, test_size=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_inputs), len(validation_inputs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"7695 + 856","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert all of our data into torch tensors, the required datatype for our model\n\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay_rate': 0.01},\n                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = BertAdam(optimizer_grouped_parameters,lr=2e-5,warmup=.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USE_GPU = True\n\nif USE_GPU and torch.cuda.is_available():\n    print('using device: cuda')\nelse:\n    print('using device: cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda:0\"\nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = [] \n\n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n\n# Number of training epochs \nepochs = 2\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n  \n  \n  # Training\n  \n  # Set our model to training mode (as opposed to evaluation mode)\n  model.train()\n  \n  # Tracking variables\n  tr_loss = 0\n  nb_tr_examples, nb_tr_steps = 0, 0\n  \n  # Train the data for one epoch\n  for step, batch in enumerate(train_dataloader):\n#     Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Clear out the gradients (by default they accumulate)\n    optimizer.zero_grad()\n    # Forward pass\n    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n    train_loss_set.append(loss.item())    \n    # Backward pass\n    loss.backward()\n    # Update parameters and take a step using the computed gradient\n    optimizer.step()\n    \n    \n    # Update tracking variables\n    tr_loss += loss.item()\n    nb_tr_examples += b_input_ids.size(0)\n    nb_tr_steps += 1\n\n  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n    \n    \n  # Validation\n\n  # Put model in evaluation mode to evaluate loss on the validation set\n  model.eval()\n\n  # Tracking variables \n  eval_loss, eval_accuracy = 0, 0\n  nb_eval_steps, nb_eval_examples = 0, 0\n\n  # Evaluate data for one epoch\n  for batch in validation_dataloader:\n#     Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    \n    # Move logits and labels to CPU\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    \n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n\n  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.title(\"Training loss\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\nplt.plot(train_loss_set)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/cola-the-corpus-of-linguistic-acceptability/cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create sentence and label lists\nsentences = df.sentence.values\n\n# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\nsentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\nlabels = df.label.values\n\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n\n\nMAX_LEN = 128\n\n# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\n# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks\nattention_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n  seq_mask = [float(i>0) for i in seq]\n  attention_masks.append(seq_mask) \n\nprediction_inputs = torch.tensor(input_ids)\nprediction_masks = torch.tensor(attention_masks)\nprediction_labels = torch.tensor(labels)\n  \nbatch_size = 32  \n\n\nprediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction on test set\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \npredictions , true_labels = [], []\n\n# Predict \nfor batch in prediction_dataloader:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask, b_labels = batch\n  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n  with torch.no_grad():\n    # Forward pass, calculate logit predictions\n    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n  # Move logits and labels to CPU\n  logits = logits.detach().cpu().numpy()\n  label_ids = b_labels.to('cpu').numpy()\n  \n  # Store predictions and true labels\n  predictions.append(logits)\n  true_labels.append(label_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(predictions[0], true_labels[0])\nprint(len(predictions[0]), len(true_labels[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import and evaluate each test batch using Matthew's correlation coefficient\nfrom sklearn.metrics import matthews_corrcoef\nmatthews_set = []\n\nfor i in range(len(true_labels)):\n  matthews = matthews_corrcoef(true_labels[i],\n                 np.argmax(predictions[i], axis=1).flatten())\n  matthews_set.append(matthews)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_set = []\nfor i in range(len(true_labels)):\n  matthews = np.argmax(predictions[i], axis=1).flatten()\n  prediction_set.append(matthews)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matthews_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---------------------------","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install BeautifulSoup4","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting BeautifulSoup4\n  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n\u001b[K     |████████████████████████████████| 115 kB 405 kB/s eta 0:00:01\n\u001b[?25hCollecting soupsieve>1.2\n  Downloading soupsieve-2.2.1-py3-none-any.whl (33 kB)\nInstalling collected packages: soupsieve, BeautifulSoup4\nSuccessfully installed BeautifulSoup4-4.9.3 soupsieve-2.2.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport bs4\nimport requests\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom spacy.matcher import Matcher \nfrom spacy.tokens import Span \n\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\npd.set_option('display.max_colwidth', 200)\n%matplotlib inline","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"candidate_sentences = pd.read_csv('../input/qa-csv/qa.csv', delimiter='\\t', encoding='utf-8', index_col=0)\ncandidate_sentences.shape","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(613, 1)"},"metadata":{}}]},{"cell_type":"code","source":"candidate_sentences.columns","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Index(['Query'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"candidate_sentences['Query'].sample(5)","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"536                    socks\n585     platform espadrilles\n188               pajama top\n452    TOM FORD pencil skirt\n525            multiway bras\nName: Query, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"candidate_sentences['Query'] = candidate_sentences['Query'].fillna('')","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['Query'][:5]","metadata":{"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0                    bag\n1          Fendi handbag\n2    Fendi beige handbag\n3              backpacks\n4        black backpacks\nName: Query, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"def get_entities(sent):\n    ## chunk 1\n    ent1 = \"\"\n    ent2 = \"\"\n    sub_list = []\n    obj_list = []\n\n    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n    prv_tok_text = \"\"   # previous token in the sentence\n\n    prefix = \"\"\n    modifier = \"\"\n    subject = \"\"\n\n    for tok in nlp(sent):\n        print('text:', tok.text, 'dep:', tok.dep_, 'pos:', tok.pos_, 'tag:', tok.tag_)\n        ## chunk 2\n        # if token is a punctuation mark then move on to the next token\n        if tok.dep_ != \"punct\":\n            # check: token is a compound word or not\n            if tok.dep_ == \"compound\":\n                prefix = tok.text\n                # if the previous word was also a 'compound' then add the current word to it\n                if prv_tok_dep == \"compound\":\n                    prefix = prv_tok_text + \" \" + tok.text\n\n        # check: token is a modifier or not\n        if tok.dep_.endswith(\"mod\") == True:\n            # if the previous word was also a 'compound' then add the current word to it\n            if prv_tok_dep == \"compound\":\n                modifier = prefix + \" \" + tok.text\n                prefix = \" \"\n            elif prv_tok_dep.endswith(\"mod\"):\n                modifier = prv_tok_text + \" \" + tok.text\n            else:\n                modifier = tok.text\n\n#           ## chunk 3\n#         if tok.dep_.find(\"subj\") == True:\n#             ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n#             sub_list.append(ent1.strip())\n#             prefix = \"\"\n#             modifier = \"\"\n#             prv_tok_dep = \"\"\n#             prv_tok_text = \"\"\n        \n        if (tok.dep_.find(\"subj\") == True and tok.tag_.lower() not in ('prp')):\n            subject = modifier + \" \" + prefix + \" \" + tok.text\n            prefix = \"\"\n            modifier = \"\"\n            prv_tok_dep = \"\"\n            prv_tok_text = \"\" \n        \n        elif tok.pos_.lower() == 'propn':\n            \n\n          ## chunk 4\n        elif tok.dep_.find(\"obj\") == True or (tok.dep_ == 'ROOT' and tok.tag_.lower() not in ('vbp')):\n            ent2 = (subject + \" \" + modifier + \" \" + prefix + \" \" + tok.text).strip()\n            ent2 = \" \".join(ent2.split())\n#             print('subject:', subject, 'modifier:', modifier, 'prefix:', prefix, 'text:', tok.text)\n            obj_list.append(ent2.strip())\n            prefix = \"\"\n            modifier = \"\"\n            prv_tok_dep = \"\"\n            prv_tok_text = \"\"\n            subject = \"\"\n\n        ## chunk 5  \n        # update variables\n        else:\n            prv_tok_dep = tok.dep_\n            prv_tok_text = tok.text\n\n#     return [ent1.strip(), ent2.strip()]\n    return sub_list, obj_list","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def feature_extraction(sent):\n    ## chunk 1\n    ent1 = \"\"\n    ent2 = \"\"\n    sub_list = []\n    obj_list = []\n\n    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n    prv_tok_text = \"\"   # previous token in the sentence\n\n    prefix = \"\"\n    modifier = \"\"\n    subject = \"\"\n    doc = nlp(sent)\n    doc_len = len(doc)\n    i = 0\n    while i < doc_len:\n        tok = doc[i]\n        print('text:', tok.text, 'dep:', tok.dep_, 'pos:', tok.pos_, 'tag:', tok.tag_)\n        ## chunk 2\n        # if token is a punctuation mark then move on to the next token\n\n        # check: token is a modifier or not\n        if tok.dep_.endswith(\"mod\") == True:\n            # if the previous word was also a 'compound' then add the current word to it\n            if prv_tok_dep == \"compound\":\n                modifier += \" \" + prefix + \" \" + tok.text\n                prefix = \" \"\n            elif prv_tok_dep.endswith(\"mod\"):\n                modifier += \" \" + prv_tok_text + \" \" + tok.text\n            else:\n                modifier += \" \" + tok.text\n\n        ## chunk 3        \n        elif tok.pos_.lower() == 'propn' or tok.pos_.lower() == 'noun':\n            if tok.dep_.endswith(\"mod\"):\n                subject = prefix + \" \" + tok.text\n            else:\n                subject = modifier + \" \" + prefix + \" \" + tok.text\n            j = i + 1\n            while j < doc_len:\n                tok_next = doc[j]\n                if tok_next.pos_.lower() == 'propn' or tok_next.pos_.lower() == 'noun' :\n                    subject += \" \" + tok_next.text\n                    j += 1\n                else:\n                    break\n            i = j - 1\n            subject = subject.strip()\n            subject = \" \".join(subject.split())\n            sub_list.append(subject)\n            prefix = \"\"\n            modifier = \"\"\n            prv_tok_dep = \"\"\n            prv_tok_text = \"\"\n            subject = \"\"\n            \n        elif tok.dep_ != \"punct\":\n            # check: token is a compound word or not\n            if tok.dep_ == \"compound\":\n                prefix = tok.text\n                # if the previous word was also a 'compound' then add the current word to it\n                if prv_tok_dep == \"compound\":\n                    prefix = prv_tok_text + \" \" + tok.text\n                if prv_tok_dep.endswith(\"mod\"):\n                    prefix = modifier + \" \" + tok.text\n\n#         ## chunk 4\n#         elif tok.dep_.find(\"obj\") == True or (tok.dep_ == 'ROOT' and tok.tag_.lower() not in ('vbp')):\n#             obj = (modifier + \" \" + prefix + \" \" + tok.text).strip()\n#             obj = \" \".join(obj.split())\n#             obj_list.append(obj.strip())\n#             prefix = \"\"\n#             modifier = \"\"\n#             prv_tok_dep = \"\"\n#             prv_tok_text = \"\"\n#             subject = \"\"\n\n        ## chunk 5  \n        # update variables\n        else:\n            prv_tok_dep = tok.dep_\n            prv_tok_text = tok.text\n            modifier = \"\"\n        i += 1\n\n    return sub_list","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def unsupervised_feature_extraction(sent):\n    ## chunk 1\n    category_list = []\n    quality_list = []\n\n    prefix = \"\"\n    modifier = \"\"\n    category = \"\"\n    doc = nlp(sent)\n    doc_len = len(doc)\n    i = 0\n    while i < doc_len:\n        tok = doc[i]\n#         print('text:', tok.text, 'dep:', tok.dep_, 'pos:', tok.pos_, 'tag:', tok.tag_)\n        \n        ## chunk 3: check if token is a modifier or not\n        if tok.dep_.endswith(\"mod\") == True:\n            modifier = prefix + \" \" + tok.text\n            j = i + 1\n            while j < doc_len:\n                tok_next = doc[j]\n                if tok_next.dep_.endswith(\"mod\"):\n                    modifier += \" \" + tok_next.text\n                    j += 1\n                else:\n                    break\n            i = j - 1\n            modifier = modifier.strip()\n            modifier = \" \".join(modifier.split())\n            quality_list.append(modifier)\n            prefix = \"\"\n            modifier = \"\"\n            category = \"\"\n\n        ## chunk 2: check if token is a noun or not    \n        elif tok.pos_.lower() == 'propn' or tok.pos_.lower() == 'noun':\n            category = prefix + \" \" + tok.text\n            j = i + 1\n            while j < doc_len:\n                tok_next = doc[j]\n                if (tok_next.pos_.lower() == 'propn' or tok_next.pos_.lower() == 'noun'):\n                    category += \" \" + tok_next.text\n                    j += 1\n                else:\n                    break\n            i = j - 1\n            category = category.strip()\n            category = \" \".join(category.split())\n            category_list.append(category)\n            prefix = \"\"\n            modifier = \"\"\n            category = \"\"\n\n        \n        ## chunk 4: check if token is a coumpuund word or not\n        elif tok.dep_ != \"punct\":\n            if tok.dep_.lower() == 'compound':\n                prefix += \" \" + tok.text\n                j = i + 1\n                while j < doc_len:\n                    tok_next = doc[j]\n                    if tok_next.dep_.lower() == 'compound':\n                        prefix += \" \" + tok_next.text\n                        j += 1\n                    else:\n                        break\n        i += 1\n\n    return quality_list, category_list","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# entity_pairs = []\n\n# for i in tqdm(candidate_sentences[\"Query\"][:100]):\n#   entity_pairs.append(get_entities(i))","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"feature_extraction('TOMMY HILFIGER blue jacket')","metadata":{"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"text: TOMMY dep: compound pos: PROPN tag: NNP\ntext: blue dep: amod pos: ADJ tag: JJ\ntext: jacket dep: ROOT pos: NOUN tag: NN\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"['TOMMY HILFIGER', 'blue jacket']"},"metadata":{}}]},{"cell_type":"code","source":"unsupervised_feature_extraction('TOMMY HILFIGER blue jacket')","metadata":{"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"(['blue'], ['TOMMY HILFIGER', 'jacket'])"},"metadata":{}}]},{"cell_type":"code","source":"feature_extraction('Long Sleeve blue jacket')","metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"text: Long dep: advmod pos: ADV tag: RB\ntext: Sleeve dep: nmod pos: PROPN tag: NNP\ntext: blue dep: amod pos: ADJ tag: JJ\ntext: jacket dep: ROOT pos: NOUN tag: NN\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"['Long Sleeve blue jacket']"},"metadata":{}}]},{"cell_type":"code","source":"unsupervised_feature_extraction('I have a dark green shirt and blue jean')","metadata":{"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"(['dark green', 'blue'], ['shirt', 'jean'])"},"metadata":{}}]},{"cell_type":"code","source":"feature_extraction('I have a dark green shirt and blue jean')","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"text: I dep: nsubj pos: PRON tag: PRP\ntext: have dep: ROOT pos: AUX tag: VBP\ntext: a dep: det pos: DET tag: DT\ntext: dark dep: amod pos: ADJ tag: JJ\ntext: green dep: amod pos: ADJ tag: JJ\ntext: shirt dep: dobj pos: NOUN tag: NN\ntext: and dep: cc pos: CCONJ tag: CC\ntext: blue dep: amod pos: PROPN tag: NNP\ntext: jean dep: conj pos: PROPN tag: NNP\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['dark green shirt', 'blue jean']"},"metadata":{}}]},{"cell_type":"code","source":"get_entities('maison margiela beige argyle jumper')","metadata":{"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"text: maison dep: amod pos: PROPN tag: NNP\ntext: margiela dep: nsubj pos: PROPN tag: NNP\ntext: beige dep: amod pos: ADJ tag: JJ\ntext: argyle dep: compound pos: PROPN tag: NNP\ntext: jumper dep: ROOT pos: NOUN tag: NN\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"([], ['maison margiela beige argyle jumper'])"},"metadata":{}}]},{"cell_type":"code","source":"unsupervised_feature_extraction('maison margiela beige argyle jumper')","metadata":{"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"(['maison', 'beige'], ['margiela', 'argyle jumper'])"},"metadata":{}}]},{"cell_type":"code","source":"feature_extraction('maison margiela beige argyle jumper')","metadata":{"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"text: maison dep: amod pos: PROPN tag: NNP\ntext: beige dep: amod pos: ADJ tag: JJ\ntext: argyle dep: compound pos: PROPN tag: NNP\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"[['maison margiela', 'beige argyle jumper'], []]"},"metadata":{}}]},{"cell_type":"code","source":"feature_extraction('alexander mcqueen midi dress')","metadata":{"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"text: alexander dep: nsubj pos: PROPN tag: NNP\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"[['alexander mcqueen midi dress'], []]"},"metadata":{}}]},{"cell_type":"code","source":"# unsupervised_feature_extraction('alexander mcqueen midi dress')\nunsupervised_feature_extraction('yellow short sleeve mini dress')","metadata":{"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"(['yellow short', 'mini'], ['sleeve', 'dress'])"},"metadata":{}}]},{"cell_type":"code","source":"for tok in nlp('yellow short sleeve mini dress'):\n    print(tok.dep_, tok.pos_)","metadata":{"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"amod PROPN\namod ADJ\nROOT NOUN\namod ADJ\ndobj NOUN\n","output_type":"stream"}]},{"cell_type":"code","source":"feature_pairs = []\n\nfor i in tqdm(candidate_sentences[\"Query\"]):\n    feature_pairs.append(feature_extraction(i))","metadata":{"trusted":true},"execution_count":75,"outputs":[{"name":"stderr","text":"100%|██████████| 613/613 [00:05<00:00, 118.76it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"quality_tags_list = []\ncategory_tags_list = []\n\nfor i in tqdm(candidate_sentences[\"Query\"]):\n    quality_tags, category_tags = unsupervised_feature_extraction(i)\n    quality_tags_list.append(quality_tags)\n    category_tags_list.append(category_tags)","metadata":{"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|██████████| 613/613 [00:03<00:00, 164.19it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"feature_extraction(\"I like yellow jackets\")","metadata":{"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"text: I dep: nsubj pos: PRON tag: PRP\ntext: like dep: ROOT pos: VERB tag: VBP\ntext: yellow dep: amod pos: ADJ tag: JJ\ntext: jackets dep: dobj pos: NOUN tag: NNS\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"['yellow jackets']"},"metadata":{}}]},{"cell_type":"code","source":"unsupervised_feature_extraction(\"I like yellow jackets\")","metadata":{"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"(['yellow'], ['jackets'])"},"metadata":{}}]},{"cell_type":"code","source":"feature_extraction(\"I like combo of blue jean jackets and white tshirts\")","metadata":{"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"text: I dep: nsubj pos: PRON tag: PRP\ntext: like dep: ROOT pos: VERB tag: VBP\ntext: combo dep: dobj pos: NOUN tag: NN\ntext: of dep: prep pos: ADP tag: IN\ntext: blue dep: amod pos: ADJ tag: JJ\ntext: jean dep: compound pos: ADJ tag: JJ\ntext: jackets dep: pobj pos: NOUN tag: NNS\ntext: and dep: cc pos: CCONJ tag: CC\ntext: white dep: amod pos: ADJ tag: JJ\ntext: tshirts dep: conj pos: NOUN tag: NNS\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"['combo', 'blue jean jackets', 'white tshirts']"},"metadata":{}}]},{"cell_type":"code","source":"unsupervised_feature_extraction(\"I like combo of blue jean jackets and white tshirts\")","metadata":{"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"(['blue', 'white'], ['combo', 'jean jackets', 'tshirts'])"},"metadata":{}}]},{"cell_type":"code","source":"unsupervised_feature_extraction(\"striped knee length dress\")","metadata":{"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"(['striped'], ['knee length dress'])"},"metadata":{}}]},{"cell_type":"code","source":"for tok in nlp('striped knee length dress'):\n    print(tok.dep_, tok.pos_)","metadata":{"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"amod VERB\ncompound NOUN\ncompound NOUN\nROOT NOUN\n","output_type":"stream"}]},{"cell_type":"code","source":"for tok in nlp('Fendi handbag'):\n    print(tok.dep_, tok.pos_)","metadata":{"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"compound PROPN\nROOT NOUN\n","output_type":"stream"}]},{"cell_type":"code","source":"feature_pairs","metadata":{"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"[['bag'],\n ['Fendi handbag'],\n ['Fendi', 'beige handbag'],\n ['backpacks'],\n ['black backpacks'],\n ['blue bucket bag'],\n ['bucketbag'],\n ['crocodile print bag'],\n ['animal print bag'],\n ['tote bag'],\n ['totes'],\n ['FURLA pochette'],\n ['red CELINE crossbody'],\n ['Coach work bag'],\n ['pink coccinelle bum bag'],\n ['belt bag'],\n ['duffel bag'],\n ['faux leather wallet'],\n ['pink wallet'],\n ['Floral print midi dress'],\n ['red sheath dress'],\n ['black dress'],\n ['short sleeve', 'mini dress'],\n ['burgundy crewneck dress'],\n ['crew neck dress'],\n ['Off Shoulder Dress'],\n ['line dress'],\n ['animal print dress'],\n ['leopard print dress'],\n ['lace dress'],\n ['shirt dress'],\n ['knee length dress'],\n ['full sleeve dress'],\n ['shift sack dress'],\n ['maison margiela', 'long dress'],\n ['polka dot mini dress'],\n ['needle thread', 'black dress'],\n ['flared dress'],\n ['DRESS'],\n ['red shoulder midi dress'],\n ['alberta ferreti dress'],\n ['black maxi dress'],\n ['Bodycon'],\n ['Alice Mccall midi dress'],\n ['strapless dress'],\n ['yellow dress'],\n ['reem acra maxi dress'],\n ['Halterneck Dress'],\n ['line', 'v neck dress'],\n ['alexander mcqueen midi dress'],\n ['Denim dress'],\n ['military green jacket'],\n ['TOMMY HILFIGER', 'blue jacket'],\n ['armani leather jacket'],\n ['one shoulder jumpsuit'],\n ['High rise', 'skinny jeans'],\n ['black skinny jeans'],\n ['distressed ankle length jeans'],\n ['light wash', 'wide leg jeans'],\n ['bootcut jeans'],\n ['distressed denim shorts'],\n ['khaki shorts'],\n ['denim mini skirts'],\n ['jumpsuits'],\n ['one shoulder jumpsuit'],\n ['short sleeve playsuit'],\n ['romper'],\n ['polka dot blouse'],\n ['Brandon Maxwell Blouse'],\n ['full sleeve blouse'],\n ['short sleeve bodysuit'],\n ['diesel bodysuit'],\n ['full sleeve crop top'],\n ['NORMA KAMALI bralette'],\n ['checked shirt'],\n ['brunello cucinelli shirt'],\n ['white T', 'shirt'],\n ['Tshirt'],\n ['graphic print tshirt'],\n ['graphic white tshirt'],\n ['scoop neck T', 'shirt'],\n ['printed camisole'],\n ['black ribbed tank'],\n ['shoulder', 'top'],\n ['Gucci sweatshirt'],\n ['green blouse'],\n ['polo shirt'],\n ['halter neck top'],\n ['MARNI tube top'],\n ['yellow DIOR sweater'],\n ['VALENTINO sweater'],\n ['maison margiela', 'beige argyle jumper'],\n ['full sleeve cardigan'],\n ['full sleeve silk blouse'],\n ['crepe blouse'],\n ['turtle neck sweater'],\n ['turtleneck sweater'],\n ['cashmere sweater'],\n ['one shoulder sweater'],\n ['printed skinny jeans']]"},"metadata":{}}]},{"cell_type":"code","source":"entity_pairs[:100]","metadata":{"trusted":true},"execution_count":117,"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"[([], ['bag']),\n ([], ['Fendi handbag']),\n ([], ['Fendi beige handbag']),\n ([], ['backpacks']),\n ([], ['black backpacks']),\n ([], ['blue bucket bag']),\n ([], ['bucketbag']),\n ([], ['crocodile print bag']),\n ([], ['animal print bag']),\n ([], ['tote bag']),\n ([], ['totes']),\n ([], ['FURLA pochette']),\n ([], ['red CELINE crossbody']),\n ([], ['Coach work bag']),\n ([], ['pink coccinelle bum bag']),\n ([], ['belt bag']),\n ([], ['duffel bag']),\n ([], ['faux leather wallet']),\n ([], ['pink wallet']),\n ([], ['Floral print midi dress']),\n ([], ['red sheath dress']),\n ([], ['black dress']),\n ([], ['short sleeve', 'mini dress']),\n ([], ['burgundy crewneck dress']),\n ([], ['crew neck dress']),\n ([], ['Off', 'Shoulder Dress']),\n ([], ['line dress']),\n ([], ['animal print dress']),\n ([], ['leopard print dress']),\n ([], ['lace dress']),\n ([], ['shirt dress']),\n ([], ['striped', 'knee length dress']),\n ([], ['full sleeve dress']),\n ([], ['shift sack dress']),\n ([], ['long dress']),\n ([], ['polka dot mini dress']),\n ([], ['black dress']),\n ([], ['flared dress']),\n ([], ['DRESS']),\n ([], ['red shoulder midi dress']),\n ([], ['alberta ferreti dress']),\n ([], ['black maxi dress']),\n ([], ['Bodycon dress']),\n ([], ['Mccall midi dress']),\n ([], ['strapless dress']),\n ([], ['yellow dress']),\n ([], ['acra maxi dress']),\n ([], ['Halterneck Dress']),\n ([], ['v neck dress']),\n ([], ['alexander mcqueen', 'midi dress']),\n ([], ['Denim dress']),\n ([], ['military green jacket']),\n ([], ['TOMMY HILFIGER blue jacket']),\n ([], ['armani leather jacket']),\n ([], ['one shoulder jumpsuit']),\n ([], ['High rise skinny jeans']),\n ([], ['black skinny jeans']),\n ([], ['distressed ankle length jeans']),\n ([], ['light wash', 'wide leg jeans']),\n ([], ['bootcut jeans']),\n ([], ['distressed denim shorts']),\n ([], ['khaki shorts']),\n ([], ['denim mini skirts']),\n ([], ['printed', 'jumpsuits']),\n ([], ['one shoulder jumpsuit']),\n ([], ['short sleeve playsuit']),\n ([], ['romper']),\n ([], ['polka dot blouse']),\n ([], ['Brandon Maxwell Blouse']),\n ([], ['full sleeve blouse']),\n ([], ['short sleeve bodysuit']),\n ([], ['diesel bodysuit']),\n ([], ['full sleeve crop top']),\n ([], ['NORMA KAMALI bralette']),\n ([], ['checked shirt']),\n ([], ['brunello cucinelli shirt']),\n ([], ['white T shirt']),\n ([], ['Tshirt']),\n ([], ['graphic print tshirt']),\n ([], ['MOSCHINO white graphic tshirt']),\n ([], ['neck T shirt']),\n ([], ['printed camisole']),\n ([], ['black ribbed tank']),\n ([], ['off', 'shoulder']),\n ([], ['Gucci sweatshirt']),\n ([], ['long sleeve', 'green blouse']),\n ([], ['polo shirt']),\n ([], ['halter neck top']),\n ([], ['MARNI tube', 'top']),\n ([], ['yellow DIOR sweater']),\n ([], ['VALENTINO sweater']),\n ([], ['maison margiela beige argyle jumper']),\n ([], ['full sleeve cardigan']),\n ([], ['full sleeve silk blouse']),\n ([], ['crepe blouse']),\n ([], ['turtle neck sweater']),\n ([], ['turtleneck sweater']),\n ([], ['cashmere sweater']),\n ([], ['one shoulder sweater']),\n ([], ['printed', 'skinny jeans'])]"},"metadata":{}}]},{"cell_type":"code","source":"candidate_sentences['Query'][:50]","metadata":{"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"0                                bag\n1                      Fendi handbag\n2                Fendi beige handbag\n3                          backpacks\n4                    black backpacks\n5                    blue bucket bag\n6                          bucketbag\n7                crocodile print bag\n8                   animal print bag\n9                           tote bag\n10                             totes\n11                    FURLA pochette\n12             red CELINE crossbody \n13                    Coach work bag\n14           pink coccinelle bum bag\n15                          belt bag\n16               Burberry duffel bag\n17               faux leather wallet\n18                  pale pink wallet\n19           Floral print midi dress\n20                  red sheath dress\n21                little black dress\n22    yellow short sleeve mini dress\n23           burgundy crewneck dress\n24                   crew neck dress\n25                Off Shoulder Dress\n26                      A line dress\n27                animal print dress\n28               leopard print dress\n29                        lace dress\n30                       shirt dress\n31         striped knee length dress\n32                 full sleeve dress\n33                  shift sack dress\n34        maison margiela long dress\n35              polka dot mini dress\n36       needle & thread black dress\n37                      flared dress\n38                             DRESS\n39       red off shoulder midi dress\n40             alberta ferreti dress\n41                  black maxi dress\n42                     Bodycon dress\n43           Alice Mccall midi dress\n44                   strapless dress\n45                      yellow dress\n46              reem acra maxi dress\n47                  Halterneck Dress\n48               a line v neck dress\n49      alexander mcqueen midi dress\nName: Query, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"determinants = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(candidate_sentences[\"Query\"]):\n    local_dep = []\n    for tok in nlp(i):\n        local_dep.append(tok.dep_)\n    determinants.append(local_dep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"determinants[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['Query'][:20]","metadata":{"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"0                         bag\n1               Fendi handbag\n2         Fendi beige handbag\n3                   backpacks\n4             black backpacks\n5             blue bucket bag\n6                   bucketbag\n7         crocodile print bag\n8            animal print bag\n9                    tote bag\n10                      totes\n11             FURLA pochette\n12      red CELINE crossbody \n13             Coach work bag\n14    pink coccinelle bum bag\n15                   belt bag\n16        Burberry duffel bag\n17        faux leather wallet\n18           pale pink wallet\n19    Floral print midi dress\nName: Query, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"entity_pairs[:50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_entities('TOMMY HILFIGER blue jacket')","metadata":{"trusted":true},"execution_count":101,"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"['', 'HILFIGER blue TOMMY HILFIGER jacket']"},"metadata":{}}]},{"cell_type":"code","source":"candidate_sentences['features'] = feature_pairs","metadata":{"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/training_models","metadata":{"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"candidate_sentences.to_csv('/kaggle/working/training_models/feature_extraction.csv')","metadata":{"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"quality_category_pairs[:30]","metadata":{"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"[[[], ['bag']],\n [[], ['Fendi handbag']],\n [['beige'], ['Fendi', 'handbag']],\n [[], ['backpacks']],\n [['black'], ['backpacks']],\n [['blue'], ['bucket bag']],\n [[], ['bucketbag']],\n [[], ['crocodile print bag']],\n [[], ['animal print bag']],\n [[], ['tote bag']],\n [[], ['totes']],\n [['FURLA'], ['pochette']],\n [['red'], ['CELINE crossbody']],\n [[], ['Coach work bag']],\n [['pink'], ['coccinelle bum bag']],\n [[], ['belt bag']],\n [['Burberry duffel'], ['bag']],\n [[], ['faux leather wallet']],\n [['pale pink'], ['wallet']],\n [['Floral'], ['print midi dress']],\n [['red'], ['sheath dress']],\n [['little black'], ['dress']],\n [['yellow short', 'mini'], ['sleeve', 'dress']],\n [[], ['burgundy crewneck dress']],\n [[], ['crew neck dress']],\n [[], ['Off Shoulder Dress']],\n [[], ['line dress']],\n [[], ['animal print dress']],\n [[], ['leopard print dress']],\n [[], ['lace dress']]]"},"metadata":{}}]},{"cell_type":"code","source":"candidate_sentences['Query'][:30]","metadata":{"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"0                                bag\n1                      Fendi handbag\n2                Fendi beige handbag\n3                          backpacks\n4                    black backpacks\n5                    blue bucket bag\n6                          bucketbag\n7                crocodile print bag\n8                   animal print bag\n9                           tote bag\n10                             totes\n11                    FURLA pochette\n12             red CELINE crossbody \n13                    Coach work bag\n14           pink coccinelle bum bag\n15                          belt bag\n16               Burberry duffel bag\n17               faux leather wallet\n18                  pale pink wallet\n19           Floral print midi dress\n20                  red sheath dress\n21                little black dress\n22    yellow short sleeve mini dress\n23           burgundy crewneck dress\n24                   crew neck dress\n25                Off Shoulder Dress\n26                      A line dress\n27                animal print dress\n28               leopard print dress\n29                        lace dress\nName: Query, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"candidate_sentences.columns","metadata":{"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"Index(['Query'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"quality_tags_list[:5]","metadata":{"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"['beige', 'black', 'blue', 'FURLA', 'red']"},"metadata":{}}]},{"cell_type":"code","source":"candidate_sentences['Quality Tags'] = quality_tags_list\ncandidate_sentences['Category Tags'] = category_tags_list","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"quality_category_pairs[:10]","metadata":{"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-644588f00eb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquality_category_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'quality_category_pairs' is not defined"],"ename":"NameError","evalue":"name 'quality_category_pairs' is not defined","output_type":"error"}]},{"cell_type":"code","source":"candidate_sentences.columns","metadata":{"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Index(['Query', 'Quality Tags', 'Category Tags'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"!mkdir ./dev_csvs","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"candidate_sentences.to_csv('./dev_csvs/quality_category_extraction.csv', sep='\\t', encoding='utf-8')","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"color_dataset = pd.read_csv('../input/color-rgb-dataset/colours_rgb_shades.csv')","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"color_dataset.columns","metadata":{"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Index(['Color Name', 'Credits', 'R;G;B Dec', 'RGB Hex', 'CSS Hex',\n       'BG/FG color sample'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"color_dataset['Color Name'] = color_dataset['Color Name'].fillna('')","metadata":{"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"color_names = []\nfor color in color_dataset['Color Name']:\n    color_names.append(''.join(' ' + c if c.isupper() else c for c in color).lower().strip())","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"color_names[:10]","metadata":{"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"['grey',\n 'grey,  silver',\n 'grey',\n 'light gray',\n 'light slate grey',\n 'slate gray',\n 'slate gray1',\n 'slate gray2',\n 'slate gray3',\n 'slate gray4']"},"metadata":{}}]},{"cell_type":"code","source":"color_dataset['Color Name'][:10]","metadata":{"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0              Grey\n1      Grey, Silver\n2              grey\n3         LightGray\n4    LightSlateGrey\n5         SlateGray\n6        SlateGray1\n7        SlateGray2\n8        SlateGray3\n9        SlateGray4\nName: Color Name, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"len(color_names)","metadata":{"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"660"},"metadata":{}}]},{"cell_type":"code","source":"color_dataset['Color Names Cleaned'] = color_names","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"color_dataset.to_csv('color_names_and_codes.csv', sep='\\t', encoding='utf-8')","metadata":{"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"a = 1","metadata":{"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"check_df = pd.read_csv('../input/check-set/check1.csv', header=0)","metadata":{"trusted":true},"execution_count":36,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-8d57759833c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/check-set/check1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2059\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 161, saw 2\n"],"ename":"ParserError","evalue":"Error tokenizing data. C error: Expected 1 fields in line 161, saw 2\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
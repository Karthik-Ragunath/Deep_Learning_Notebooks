{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install BeautifulSoup4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport bs4\nimport requests\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom spacy.matcher import Matcher \nfrom spacy.tokens import Span \n\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\npd.set_option('display.max_colwidth', 200)\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences = pd.read_csv(\"../input/wiki-sentences/wiki_sentences_v2.csv\")\ncandidate_sentences.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['sentence'].sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc = nlp(\"the drawdown process is governed by astm standard d823\")\n\nfor tok in doc:\n  print(tok.text, \"...\", tok.dep_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_entities(sent):\n  ## chunk 1\n  ent1 = \"\"\n  ent2 = \"\"\n\n  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n  prv_tok_text = \"\"   # previous token in the sentence\n\n  prefix = \"\"\n  modifier = \"\"\n\n  #############################################################\n  \n  for tok in nlp(sent):\n    ## chunk 2\n    # if token is a punctuation mark then move on to the next token\n    if tok.dep_ != \"punct\":\n      # check: token is a compound word or not\n      if tok.dep_ == \"compound\":\n        prefix = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          prefix = prv_tok_text + \" \"+ tok.text\n      \n      # check: token is a modifier or not\n      if tok.dep_.endswith(\"mod\") == True:\n        modifier = tok.text\n        # if the previous word was also a 'compound' then add the current word to it\n        if prv_tok_dep == \"compound\":\n          modifier = prv_tok_text + \" \"+ tok.text\n      \n      ## chunk 3\n      if tok.dep_.find(\"subj\") == True:\n        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n        prefix = \"\"\n        modifier = \"\"\n        prv_tok_dep = \"\"\n        prv_tok_text = \"\"      \n\n      ## chunk 4\n      if tok.dep_.find(\"obj\") == True:\n        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n        \n      ## chunk 5  \n      # update variables\n      prv_tok_dep = tok.dep_\n      prv_tok_text = tok.text\n  #############################################################\n\n  return [ent1.strip(), ent2.strip()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_entities(\"the film had 200 patents\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"entity_pairs = []\n\nfor i in tqdm(candidate_sentences[\"sentence\"]):\n  entity_pairs.append(get_entities(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"entity_pairs[10:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_relation(sent):\n\n  doc = nlp(sent)\n\n#   for tok in doc:\n#     print(tok.text, \" \", tok.dep_, \" \", tok.pos_)\n\n  # Matcher class object \n  matcher = Matcher(nlp.vocab)\n\n  #define the pattern \n  pattern = [{'DEP':'ROOT'}, \n            {'DEP':'prep','OP':\"?\"},\n            {'DEP':'agent','OP':\"?\"},  \n            {'POS':'ADJ','OP':\"?\"}] \n\n  matcher.add(\"matching_1\", None, pattern) \n\n  matches = matcher(doc)\n#   print(\"Matches:\", matches)\n  k = len(matches) - 1\n#   print(\"K:\", k)\n  span = doc[matches[k][1]:matches[k][2]] \n\n  return(span.text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_relation(\"John completed the task\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_relation(\"John and Jan\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relations = [get_relation(i) for i in tqdm(candidate_sentences['sentence'])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series(relations).value_counts()[:50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract subject\nsource = [i[0] for i in entity_pairs]\n\n# extract object\ntarget = [i[1] for i in entity_pairs]\n\nkg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kg_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,12))\n\npos = nx.spring_layout(G)\nnx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"composed by\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"released in\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5)\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-pretrained-bert pytorch-nlp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Libraries\n\nimport tensorflow as tf\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertConfig\nfrom pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport io\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/cola-the-corpus-of-linguistic-acceptability/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create sentence and label lists\nsentences = df.sentence.values\n\n# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\nsentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\nlabels = df.label.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\nprint (\"Tokenize the first sentence:\")\nprint (tokenized_texts[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(input_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(input_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create attention masks\nattention_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n  seq_mask = [float(i>0) for i in seq]\n  attention_masks.append(seq_mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attention_masks[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.label_notes.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use train_test_split to split our data into train and validation sets for training\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n                                                            random_state=2018, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                             random_state=2018, test_size=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_inputs), len(validation_inputs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"7695 + 856","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert all of our data into torch tensors, the required datatype for our model\n\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_optimizer = list(model.named_parameters())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('==== Embedding Layer ====\\n')\nfor p in param_optimizer[0:5]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\n==== First Transformer ====\\n')\n\nfor p in param_optimizer[5:21]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\n\nfor p in param_optimizer[-4:]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for n, p in param_optimizer:\n    print(n)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# no_decay = ['bias', 'gamma', 'beta']\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay_rate': 0.01},\n                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(optimizer_grouped_parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = BertAdam(optimizer_grouped_parameters,lr=2e-5,warmup=.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USE_GPU = True\n\nif USE_GPU and torch.cuda.is_available():\n    print('using device: cuda')\nelse:\n    print('using device: cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda:0\"\nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = [] \n\n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n\n# Number of training epochs \nepochs = 2\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n  \n  \n  # Training\n  \n  # Set our model to training mode (as opposed to evaluation mode)\n  model.train()\n  \n  # Tracking variables\n  tr_loss = 0\n  nb_tr_examples, nb_tr_steps = 0, 0\n  \n  # Train the data for one epoch\n  for step, batch in enumerate(train_dataloader):\n#     Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Clear out the gradients (by default they accumulate)\n    optimizer.zero_grad()\n    # Forward pass\n    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n    train_loss_set.append(loss.item())    \n    # Backward pass\n    loss.backward()\n    # Update parameters and take a step using the computed gradient\n    optimizer.step()\n    \n    \n    # Update tracking variables\n    tr_loss += loss.item()\n    nb_tr_examples += b_input_ids.size(0)\n    nb_tr_steps += 1\n\n  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n    \n    \n  # Validation\n\n  # Put model in evaluation mode to evaluate loss on the validation set\n  model.eval()\n\n  # Tracking variables \n  eval_loss, eval_accuracy = 0, 0\n  nb_eval_steps, nb_eval_examples = 0, 0\n\n  # Evaluate data for one epoch\n  for batch in validation_dataloader:\n#     Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    \n    # Move logits and labels to CPU\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    \n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n\n  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.title(\"Training loss\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\nplt.plot(train_loss_set)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/cola-the-corpus-of-linguistic-acceptability/cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create sentence and label lists\nsentences = df.sentence.values\n\n# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\nsentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\nlabels = df.label.values\n\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n\n\nMAX_LEN = 128\n\n# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\n# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks\nattention_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n  seq_mask = [float(i>0) for i in seq]\n  attention_masks.append(seq_mask) \n\nprediction_inputs = torch.tensor(input_ids)\nprediction_masks = torch.tensor(attention_masks)\nprediction_labels = torch.tensor(labels)\n  \nbatch_size = 32  \n\n\nprediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction on test set\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \npredictions , true_labels = [], []\n\n# Predict \nfor batch in prediction_dataloader:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask, b_labels = batch\n  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n  with torch.no_grad():\n    # Forward pass, calculate logit predictions\n    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n  # Move logits and labels to CPU\n  logits = logits.detach().cpu().numpy()\n  label_ids = b_labels.to('cpu').numpy()\n  \n  # Store predictions and true labels\n  predictions.append(logits)\n  true_labels.append(label_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(predictions[0], true_labels[0])\nprint(len(predictions[0]), len(true_labels[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import and evaluate each test batch using Matthew's correlation coefficient\nfrom sklearn.metrics import matthews_corrcoef\nmatthews_set = []\n\nfor i in range(len(true_labels)):\n  matthews = matthews_corrcoef(true_labels[i],\n                 np.argmax(predictions[i], axis=1).flatten())\n  matthews_set.append(matthews)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_set = []\nfor i in range(len(true_labels)):\n  matthews = np.argmax(predictions[i], axis=1).flatten()\n  prediction_set.append(matthews)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matthews_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---------------------------","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install BeautifulSoup4","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting BeautifulSoup4\n  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n\u001b[K     |████████████████████████████████| 115 kB 905 kB/s eta 0:00:01\n\u001b[?25hCollecting soupsieve>1.2\n  Downloading soupsieve-2.2.1-py3-none-any.whl (33 kB)\nInstalling collected packages: soupsieve, BeautifulSoup4\nSuccessfully installed BeautifulSoup4-4.9.3 soupsieve-2.2.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport bs4\nimport requests\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom spacy.matcher import Matcher \nfrom spacy.tokens import Span \n\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\npd.set_option('display.max_colwidth', 200)\n%matplotlib inline","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"candidate_sentences = pd.read_csv('../input/qa-csv/qa.csv', delimiter='\\t', encoding='utf-8', index_col=0)\ncandidate_sentences.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['Query'].sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['Query'] = candidate_sentences['Query'].fillna('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['Query'][:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_entities(sent):\n#     ## chunk 1\n#     ent1 = \"\"\n#     ent2 = \"\"\n#     sub_list = []\n#     obj_list = []\n\n#     prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n#     prv_tok_text = \"\"   # previous token in the sentence\n\n#     prefix = \"\"\n#     modifier = \"\"\n#     subject = \"\"\n\n#     for tok in nlp(sent):\n#         print('text:', tok.text, 'dep:', tok.dep_, 'pos:', tok.pos_, 'tag:', tok.tag_)\n#         ## chunk 2\n#         # if token is a punctuation mark then move on to the next token\n#         if tok.dep_ != \"punct\":\n#             # check: token is a compound word or not\n#             if tok.dep_ == \"compound\":\n#                 prefix = tok.text\n#                 # if the previous word was also a 'compound' then add the current word to it\n#                 if prv_tok_dep == \"compound\":\n#                     prefix = prv_tok_text + \" \" + tok.text\n\n#         # check: token is a modifier or not\n#         if tok.dep_.endswith(\"mod\") == True:\n#             # if the previous word was also a 'compound' then add the current word to it\n#             if prv_tok_dep == \"compound\":\n#                 modifier = prefix + \" \" + tok.text\n#                 prefix = \" \"\n#             elif prv_tok_dep.endswith(\"mod\"):\n#                 modifier = prv_tok_text + \" \" + tok.text\n#             else:\n#                 modifier = tok.text\n\n# #           ## chunk 3\n# #         if tok.dep_.find(\"subj\") == True:\n# #             ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n# #             sub_list.append(ent1.strip())\n# #             prefix = \"\"\n# #             modifier = \"\"\n# #             prv_tok_dep = \"\"\n# #             prv_tok_text = \"\"\n        \n#         if (tok.dep_.find(\"subj\") == True and tok.tag_.lower() not in ('prp')):\n#             subject = modifier + \" \" + prefix + \" \" + tok.text\n#             prefix = \"\"\n#             modifier = \"\"\n#             prv_tok_dep = \"\"\n#             prv_tok_text = \"\" \n        \n#         elif tok.pos_.lower() == 'propn':\n            \n\n#           ## chunk 4\n#         elif tok.dep_.find(\"obj\") == True or (tok.dep_ == 'ROOT' and tok.tag_.lower() not in ('vbp')):\n#             ent2 = (subject + \" \" + modifier + \" \" + prefix + \" \" + tok.text).strip()\n#             ent2 = \" \".join(ent2.split())\n# #             print('subject:', subject, 'modifier:', modifier, 'prefix:', prefix, 'text:', tok.text)\n#             obj_list.append(ent2.strip())\n#             prefix = \"\"\n#             modifier = \"\"\n#             prv_tok_dep = \"\"\n#             prv_tok_text = \"\"\n#             subject = \"\"\n\n#         ## chunk 5  \n#         # update variables\n#         else:\n#             prv_tok_dep = tok.dep_\n#             prv_tok_text = tok.text\n\n# #     return [ent1.strip(), ent2.strip()]\n#     return sub_list, obj_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_extraction(sent):\n    ## chunk 1\n    ent1 = \"\"\n    ent2 = \"\"\n    sub_list = []\n    obj_list = []\n\n    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n    prv_tok_text = \"\"   # previous token in the sentence\n\n    prefix = \"\"\n    modifier = \"\"\n    subject = \"\"\n    doc = nlp(sent)\n    doc_len = len(doc)\n    i = 0\n    while i < doc_len:\n        tok = doc[i]\n        print('text:', tok.text, 'dep:', tok.dep_, 'pos:', tok.pos_, 'tag:', tok.tag_)\n        ## chunk 2\n        # if token is a punctuation mark then move on to the next token\n\n        # check: token is a modifier or not\n        if tok.dep_.endswith(\"mod\") == True:\n            # if the previous word was also a 'compound' then add the current word to it\n            if prv_tok_dep == \"compound\":\n                modifier += \" \" + prefix + \" \" + tok.text\n                prefix = \" \"\n            elif prv_tok_dep.endswith(\"mod\"):\n                modifier += \" \" + prv_tok_text + \" \" + tok.text\n            else:\n                modifier += \" \" + tok.text\n\n        ## chunk 3        \n        elif tok.pos_.lower() == 'propn' or tok.pos_.lower() == 'noun':\n            if tok.dep_.endswith(\"mod\"):\n                subject = prefix + \" \" + tok.text\n            else:\n                subject = modifier + \" \" + prefix + \" \" + tok.text\n            j = i + 1\n            while j < doc_len:\n                tok_next = doc[j]\n                if tok_next.pos_.lower() == 'propn' or tok_next.pos_.lower() == 'noun' :\n                    subject += \" \" + tok_next.text\n                    j += 1\n                else:\n                    break\n            i = j - 1\n            subject = subject.strip()\n            subject = \" \".join(subject.split())\n            sub_list.append(subject)\n            prefix = \"\"\n            modifier = \"\"\n            prv_tok_dep = \"\"\n            prv_tok_text = \"\"\n            subject = \"\"\n            \n        elif tok.dep_ != \"punct\":\n            # check: token is a compound word or not\n            if tok.dep_ == \"compound\":\n                prefix = tok.text\n                # if the previous word was also a 'compound' then add the current word to it\n                if prv_tok_dep == \"compound\":\n                    prefix = prv_tok_text + \" \" + tok.text\n                if prv_tok_dep.endswith(\"mod\"):\n                    prefix = modifier + \" \" + tok.text\n\n#         ## chunk 4\n#         elif tok.dep_.find(\"obj\") == True or (tok.dep_ == 'ROOT' and tok.tag_.lower() not in ('vbp')):\n#             obj = (modifier + \" \" + prefix + \" \" + tok.text).strip()\n#             obj = \" \".join(obj.split())\n#             obj_list.append(obj.strip())\n#             prefix = \"\"\n#             modifier = \"\"\n#             prv_tok_dep = \"\"\n#             prv_tok_text = \"\"\n#             subject = \"\"\n\n        ## chunk 5  \n        # update variables\n        else:\n            prv_tok_dep = tok.dep_\n            prv_tok_text = tok.text\n            modifier = \"\"\n        i += 1\n\n    return sub_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unsupervised_feature_extraction(sent):\n    ## chunk 1\n    category_list = []\n    quality_list = []\n\n    prefix = \"\"\n    modifier = \"\"\n    category = \"\"\n    doc = nlp(sent)\n    doc_len = len(doc)\n    i = 0\n    while i < doc_len:\n        tok = doc[i]\n#         print('text:', tok.text, 'dep:', tok.dep_, 'pos:', tok.pos_, 'tag:', tok.tag_)\n        \n        ## chunk 3: check if token is a modifier or not\n        if tok.dep_.endswith(\"mod\") == True:\n            modifier = prefix + \" \" + tok.text\n            j = i + 1\n            while j < doc_len:\n                tok_next = doc[j]\n                if tok_next.dep_.endswith(\"mod\"):\n                    modifier += \" \" + tok_next.text\n                    j += 1\n                else:\n                    break\n            i = j - 1\n            modifier = modifier.strip()\n            modifier = \" \".join(modifier.split())\n            quality_list.append(modifier)\n            prefix = \"\"\n            modifier = \"\"\n            category = \"\"\n\n        ## chunk 2: check if token is a noun or not    \n        elif tok.pos_.lower() == 'propn' or tok.pos_.lower() == 'noun':\n            category = prefix + \" \" + tok.text\n            j = i + 1\n            while j < doc_len:\n                tok_next = doc[j]\n                if (tok_next.pos_.lower() == 'propn' or tok_next.pos_.lower() == 'noun'):\n                    category += \" \" + tok_next.text\n                    j += 1\n                else:\n                    break\n            i = j - 1\n            category = category.strip()\n            category = \" \".join(category.split())\n            category_list.append(category)\n            prefix = \"\"\n            modifier = \"\"\n            category = \"\"\n\n        \n        ## chunk 4: check if token is a coumpuund word or not\n        elif tok.dep_ != \"punct\":\n            if tok.dep_.lower() == 'compound':\n                prefix += \" \" + tok.text\n                j = i + 1\n                while j < doc_len:\n                    tok_next = doc[j]\n                    if tok_next.dep_.lower() == 'compound':\n                        prefix += \" \" + tok_next.text\n                        j += 1\n                    else:\n                        break\n        i += 1\n\n    return quality_list, category_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# entity_pairs = []\n\n# for i in tqdm(candidate_sentences[\"Query\"][:100]):\n#   entity_pairs.append(get_entities(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extraction('TOMMY HILFIGER blue jacket')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unsupervised_feature_extraction('TOMMY HILFIGER blue jacket')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extraction('Long Sleeve blue jacket')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unsupervised_feature_extraction('I have a dark green shirt and blue jean')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extraction('I have a dark green shirt and blue jean')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get_entities('maison margiela beige argyle jumper')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unsupervised_feature_extraction('maison margiela beige argyle jumper')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extraction('maison margiela beige argyle jumper')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extraction('alexander mcqueen midi dress')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unsupervised_feature_extraction('alexander mcqueen midi dress')\nunsupervised_feature_extraction('yellow short sleeve mini dress')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for tok in nlp('yellow short sleeve mini dress'):\n    print(tok.dep_, tok.pos_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature_pairs = []\n\n# for i in tqdm(candidate_sentences[\"Query\"]):\n#     feature_pairs.append(feature_extraction(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quality_tags_list = []\ncategory_tags_list = []\n\nfor i in tqdm(candidate_sentences[\"Query\"]):\n    quality_tags, category_tags = unsupervised_feature_extraction(i)\n    quality_tags_list.append(quality_tags)\n    category_tags_list.append(category_tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extraction(\"I like yellow jackets\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unsupervised_feature_extraction(\"I like yellow jackets\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extraction(\"I like combo of blue jean jackets and white tshirts\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unsupervised_feature_extraction(\"I like combo of blue jean jackets and white tshirts\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unsupervised_feature_extraction(\"striped knee length dress\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for tok in nlp('striped knee length dress'):\n    print(tok.dep_, tok.pos_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for tok in nlp('Fendi handbag'):\n    print(tok.dep_, tok.pos_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_pairs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"entity_pairs[:100]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['Query'][:50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"determinants = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(candidate_sentences[\"Query\"]):\n    local_dep = []\n    for tok in nlp(i):\n        local_dep.append(tok.dep_)\n    determinants.append(local_dep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"determinants[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['Query'][:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"entity_pairs[:50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_entities('TOMMY HILFIGER blue jacket')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['features'] = feature_pairs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/training_models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences.to_csv('/kaggle/working/training_models/feature_extraction.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quality_category_pairs[:30]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['Query'][:30]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quality_tags_list[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['Quality Tags'] = quality_tags_list\ncandidate_sentences['Category Tags'] = category_tags_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quality_category_pairs[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir ./dev_csvs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences.to_csv('./dev_csvs/quality_category_extraction.csv', sep='\\t', encoding='utf-8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_dataset = pd.read_csv('../input/color-rgb-dataset/colours_rgb_shades.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_dataset.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_dataset['Color Name'] = color_dataset['Color Name'].fillna('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_names = []\nfor color in color_dataset['Color Name']:\n    color_names.append(''.join(' ' + c if c.isupper() else c for c in color).lower().strip())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_names[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_dataset['Color Name'][:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(color_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_dataset['Color Names Cleaned'] = color_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_dataset.to_csv('color_names_and_codes.csv', sep='\\t', encoding='utf-8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_df = pd.read_csv('../input/datacleaner/data_cleanser.csv', header=0, delimiter='\\t', error_bad_lines=False)","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"b'Skipping line 1114: expected 1 fields, saw 2\\nSkipping line 2126: expected 1 fields, saw 2\\nSkipping line 2296: expected 1 fields, saw 2\\nSkipping line 4922: expected 1 fields, saw 2\\nSkipping line 9500: expected 1 fields, saw 2\\nSkipping line 12578: expected 1 fields, saw 2\\nSkipping line 15295: expected 1 fields, saw 2\\nSkipping line 18309: expected 1 fields, saw 2\\nSkipping line 21193: expected 1 fields, saw 2\\nSkipping line 24723: expected 1 fields, saw 2\\nSkipping line 35928: expected 1 fields, saw 2\\nSkipping line 38161: expected 1 fields, saw 2\\nSkipping line 47049: expected 1 fields, saw 2\\nSkipping line 51626: expected 1 fields, saw 2\\n'\n","output_type":"stream"}]},{"cell_type":"code","source":"check_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(check_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_names_set = set(color_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if 'dark green' in color_names:\n    print('yes')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 0\nfor ind_color in color_names:\n    comma_sep_colors = ind_color.split(',')\n    for color in comma_sep_colors:\n        color = ' '.join(color.split())\n        try:\n            length = len(color.split(' '))\n        except:\n            print(color, comma_sep_colors)\n        if length > max_len:\n            max_len = length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"color_dictionary = defaultdict(list)\nfor ind_color in color_names:\n    comma_sep_colors = ind_color.split(',')\n    for color in comma_sep_colors:\n        color = ' '.join(color.split())\n        split_color = color.split(' ')\n        length = len(split_color)\n        for i in range(length):\n            color_dictionary[i + 1].append(' '.join(split_color[0:(i + 1)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_dictionary[3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_dictionary[5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_df['data_checker'] = check_df['data_checker'].fillna('')","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"data_checker_list = list(check_df['data_checker'])","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"max_len = 0\nfor ind_data in data_checker_list:\n    comma_sep_data = ind_data.split(',')\n    for datum in comma_sep_data:\n        datum = ' '.join(datum.split())\n        try:\n            length = len(datum.split(' '))\n        except:\n            print(datum, comma_sep_data)\n        if length > max_len:\n            max_len = length","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"max_len","metadata":{"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"11"},"metadata":{}}]},{"cell_type":"code","source":"data_checker_dict = defaultdict(list)\nmax_len = 0\nfor ind_data in data_checker_list:\n    comma_sep_data = ind_data.split(',')\n    for datum in comma_sep_data:\n        datum = ' '.join(datum.split())\n        datum_split = datum.split(' ')\n        length = len(datum_split)\n        for i in range(length):\n            data_checker_dict[i + 1].append(' '.join(datum_split[0:(i + 1)]))","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"data_checker_dict[2][:5]","metadata":{"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['E-Land American', 'JoJo Maman', 'Souris Mini', 'Janie and', 'CCH Collection']"},"metadata":{}}]},{"cell_type":"code","source":"def unsupervised_feature_extraction(sent):\n    ## chunk 1\n    category_list = []\n    quality_list = []\n\n    prefix = \"\"\n    modifier = \"\"\n    category = \"\"\n    doc = nlp(sent)\n    doc_len = len(doc)\n    i = 0\n    while i < doc_len:\n        tok = doc[i]\n#         print('text:', tok.text, 'dep:', tok.dep_, 'pos:', tok.pos_, 'tag:', tok.tag_)\n        \n        ## chunk 3: check if token is a modifier or not\n        if tok.dep_.endswith(\"mod\") == True:\n            modifier = prefix + \" \" + tok.text\n            j = i + 1\n            while j < doc_len:\n                tok_next = doc[j]\n                if tok_next.dep_.endswith(\"mod\"):\n                    modifier += \" \" + tok_next.text\n                    j += 1\n                else:\n                    break\n            i = j - 1\n            modifier = modifier.strip()\n            modifier = \" \".join(modifier.split())\n            quality_list.append(modifier)\n            prefix = \"\"\n            modifier = \"\"\n            category = \"\"\n\n        ## chunk 2: check if token is a noun or not    \n        elif tok.pos_.lower() == 'propn' or tok.pos_.lower() == 'noun':\n            category = prefix + \" \" + tok.text\n            j = i + 1\n            while j < doc_len:\n                tok_next = doc[j]\n                if (tok_next.pos_.lower() == 'propn' or tok_next.pos_.lower() == 'noun'):\n                    category += \" \" + tok_next.text\n                    j += 1\n                else:\n                    break\n            i = j - 1\n            category = category.strip()\n            category = \" \".join(category.split())\n            category_list.append(category)\n            prefix = \"\"\n            modifier = \"\"\n            category = \"\"\n\n        \n        ## chunk 4: check if token is a coumpuund word or not\n        elif tok.dep_ != \"punct\":\n            if tok.dep_.lower() == 'compound':\n                prefix += \" \" + tok.text\n                j = i + 1\n                while j < doc_len:\n                    tok_next = doc[j]\n                    if tok_next.dep_.lower() == 'compound':\n                        prefix += \" \" + tok_next.text\n                        j += 1\n                    else:\n                        break\n        i += 1\n\n    return quality_list, category_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quality_tags_list = []\ncategory_tags_list = []\nbrand_list = []\nfor k in tqdm(candidate_sentences[\"Query\"]):\n    sentence_split = k.split()\n    split_len = len(sentence_split)\n    brand_str = \"\"\n    for i in range(split_len):\n        if sentence_split[i] in data_checker_dict[1]:\n#             print('1:', sentence_split[i])\n#             print(k)\n            start_index = i\n            j = 2\n            loop_index = i + 1\n            while loop_index < split_len and sentence_split[loop_index] in data_checker_dict[j]:\n                j += 1\n                loop_index += 1\n            end_index = loop_index - 1\n#             print(start_index, end_index)\n            brand_str = ' '.join(sentence_split[start_index:(end_index+1)])\n            sentence_split[start_index:(end_index+1)] = []\n            k = ' '.join(sentence_split)\n#             print(k)\n            break\n    brand_list.append(brand_str)\n#     quality_tags, category_tags = unsupervised_feature_extraction(i)\n#     quality_tags_list.append(quality_tags)\n#     category_tags_list.append(category_tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(brand_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(candidate_sentences[\"Query\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"brand_set = set(brand_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"brand_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'white' in data_checker_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 0\nfor ind_color in color_names:\n    comma_sep_colors = ind_color.split(',')\n    for color in comma_sep_colors:\n        color = ' '.join(color.split())\n        try:\n            length = len(color.split(' '))\n        except:\n            print(color, comma_sep_colors)\n        if length > max_len:\n            max_len = length\n\ncolor_dictionary = defaultdict(set)\ncolor_set = set(color_names)\nfor ind_color in color_names:\n    comma_sep_colors = ind_color.split(',')\n    for color in comma_sep_colors:\n        color = ' '.join(color.split())\n        split_color = color.split(' ')\n        length = len(split_color)\n        initial_key = \"\"\n        for i in range(length):\n            color_dictionary[initial_key].add(' '.join(split_color[0:(i + 1)]))\n            initial_key = ' '.join(split_color[0:(i + 1)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_df['data_checker'] = check_df['data_checker'].fillna('')\ndata_checker_list = list(check_df['data_checker'])\nmax_len = 0\nfor ind_data in data_checker_list:\n    comma_sep_data = ind_data.split(',')\n    for datum in comma_sep_data:\n        datum = ' '.join(datum.split())\n        try:\n            length = len(datum.split(' '))\n        except:\n            print(datum, comma_sep_data)\n        if length > max_len:\n            max_len = length\n\ndata_checker_dict = defaultdict(set)\ndata_checker_set = set(data_checker_list)\nwords_to_remove = ['Dress', 'Skirt', 'Black']\ndata_checker_set = set(filter(lambda x:(len(x)!=1 and x not in words_to_remove), data_checker_set))\nfor ind_data in data_checker_list:\n    comma_sep_data = ind_data.split(',')\n    for datum in comma_sep_data:\n        datum = ' '.join(datum.split())\n        datum_split = datum.split(' ')\n        length = len(datum_split)\n        initial_key = \"\"\n        for i in range(length):\n            data_checker_dict[initial_key].add(' '.join(datum_split[0:(i + 1)]))\n            initial_key = ' '.join(datum_split[0:(i + 1)])","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"len(data_checker_set)","metadata":{"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"54741"},"metadata":{}}]},{"cell_type":"code","source":"len(data_checker_dict.keys())","metadata":{"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"33574"},"metadata":{}}]},{"cell_type":"code","source":"(color_dictionary.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quality_tags_list = []\ncategory_tags_list = []\nbrand_list = []\nfor k in tqdm(candidate_sentences[\"Query\"]):\n    sentence_split = k.split()\n    split_len = len(sentence_split)\n    brand_str = \"\"\n    initial_key = \"\"\n    \n    for i in range(split_len):\n        if sentence_split[i] in data_checker_dict[\"\"]:\n            start_index = i\n            j = 2\n            loop_index = i + 1\n            brand_tuple_index = tuple()\n            while loop_index <= split_len and \" \".join(sentence_split[start_index:loop_index]) in data_checker_dict[\" \".join(sentence_split[start_index:(loop_index - 1)])]:\n                if \" \".join(sentence_split[start_index:loop_index]) in data_checker_set:\n                    brand_tuple_index = (start_index, loop_index) \n                j += 1\n                loop_index += 1\n            if brand_tuple_index:\n                brand_str = ' '.join(sentence_split[brand_tuple_index[0]:brand_tuple_index[1]])\n                brand_list.append(brand_str)\n                print('brand:', brand_str, 'sentence:', k)\n                sentence_split[brand_tuple_index[0]:brand_tuple_index[1]] = []\n                break\n    if not brand_str:\n        brand_list.append(\"\")\n    k = ' '.join(sentence_split)\n    quality_tags, category_tags = unsupervised_feature_extraction(k)\n    quality_tags_list.append(quality_tags)\n    category_tags_list.append(category_tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'A' in data_checker_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = ['a', 'b']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\" \".join(a[0:0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checker = defaultdict(set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checker['a'].add('v')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = tuple()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = (1,2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = ['a']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a[0:1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"brand_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'Alexander Mcqueen' in data_checker_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myset = set(['a','b', 'ab'])\nmyset = set(filter(lambda x:len(x)!=1, myset))\nprint(myset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_sentences['Quality Tags'] = quality_tags_list\ncandidate_sentences['Category Tags'] = category_tags_list\ncandidate_sentences['Brand List'] = brand_list\n!mkdir ./dev_csvs\ncandidate_sentences.to_csv('./dev_csvs/quality_category_brand_extraction.csv', sep='\\t', encoding='utf-8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = {'1':['2']}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\njson.dumps(d)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame(columns=['tags'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('./current_tagger.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('./current_tagger.csv', header=None, names=['tags'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.append({'tags':json.dumps(d)}, ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('./current_tagger.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('./current_tagger.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------ Entity Recognition","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"LG Premium 50L cooker under 50000\"\n# text = \"LG Premium Cooker with capacity under 30L\"\n# text = 'yellow short sleeve mini dress'\n# text = 'yellow short sleeve mini dress between 1000 and 2000'\n# text = \"LG Premium cooker under 50 dollars\"\n# text = \"LG Premium cooker under 50 dollars with discount\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for tok in doc:\n    print(tok.text, tok.dep_, tok.pos_)\nprint(doc.ents)\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unsupervised_feature_extraction(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_preposition_meaning(token, cur_string):\n    for child_val in token.children:\n        if child_val in token.rights:\n            cur_string += \" \" + child_val.text\n        elif child_val in token.lefts:\n            cur_string = child_val.text + \" \" + cur_string\n        cur_string = cur_string.strip()\n        if child_val.children:\n            cur_string = get_preposition_meaning(child_val, cur_string)\n    return cur_string","metadata":{"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"for token in doc:\n    print(token.text, token.dep_, token.head.text, token.head.pos_,\n            [child for child in token.children])\n    if token.dep_ == 'prep':\n#         print('rights:', token.rights)\n#         for right_val in token.rights:\n#             print('val:', right_val)\n#         for descendent in token.subtree:\n#             print('descendent:', descendent, token.is_ancestor(descendent))\n#         print(token.n_rights)\n        cur_string = \"\"\n        prep_meaning = get_preposition_meaning(token, cur_string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prep_meaning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for tok in doc:\n    for child in tok.children:\n        print('child:', child, 'type:', type(child))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unsupervised_feature_extraction_dependency_extraction(sent):\n    ## chunk 1\n    category_list = []\n    quality_list = []\n    preposition_list = []\n    preposition_meaning_list = []\n    \n    prefix = \"\"\n    modifier = \"\"\n    category = \"\"\n    doc = nlp(sent)\n    doc_len = len(doc)\n    i = 0\n    while i < doc_len:\n        tok = doc[i]\n#         print('text:', tok.text, 'dep:', tok.dep_, 'pos:', tok.pos_, 'tag:', tok.tag_)\n        \n        ## chunk 3: check if token is a modifier or not\n        if tok.dep_.endswith(\"mod\") == True:\n            modifier = prefix + \" \" + tok.text\n            j = i + 1\n            while j < doc_len:\n                tok_next = doc[j]\n                if tok_next.dep_.endswith(\"mod\"):\n                    modifier += \" \" + tok_next.text\n                    j += 1\n                else:\n                    break\n            i = j - 1\n            modifier = modifier.strip()\n            modifier = \" \".join(modifier.split())\n            quality_list.append(modifier)\n            prefix = \"\"\n            modifier = \"\"\n            category = \"\"\n\n        ## chunk 2: check if token is a noun or not    \n        elif tok.pos_.lower() == 'propn' or tok.pos_.lower() == 'noun':\n            category = prefix + \" \" + tok.text\n            j = i + 1\n            while j < doc_len:\n                tok_next = doc[j]\n                if (tok_next.pos_.lower() == 'propn' or tok_next.pos_.lower() == 'noun'):\n                    category += \" \" + tok_next.text\n                    j += 1\n                else:\n                    break\n            i = j - 1\n            category = category.strip()\n            category = \" \".join(category.split())\n            category_list.append(category)\n            prefix = \"\"\n            modifier = \"\"\n            category = \"\"\n\n        \n        ## chunk 4: check if token is a coumpuund word or not\n        elif tok.dep_ != \"punct\" and tok.dep_.lower() == 'compound':\n            prefix += \" \" + tok.text\n            j = i + 1\n            while j < doc_len:\n                tok_next = doc[j]\n                if tok_next.dep_.lower() == 'compound':\n                    prefix += \" \" + tok_next.text\n                    j += 1\n                else:\n                    break\n                    \n        elif tok.dep_ == 'prep':\n            prep_string = \"\"\n            preposition_list.append(tok.text)\n            preposition_meaning_list.append(get_preposition_meaning(tok, prep_string))\n        \n        i += 1\n\n    return quality_list, category_list, preposition_list, preposition_meaning_list","metadata":{"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"prep_df = pd.read_csv('../input/preposition-sample-test/preposition_searches.csv', index_col=0, header=0)","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"prep_df = prep_df.drop('index', axis=1)","metadata":{"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"prep_df.columns","metadata":{"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Index(['search_text'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"prep_df[:5]","metadata":{"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"                                                                              search_text\n0                                                                      'iphone under 30k'\n1                                                               samsung phones under 5000\n2                                                          earphones between 1000 to 2000\n3                      [1:37 pm] ayush pushkar     8 gb ram mobile between 10000 & 30000 \n4  [1:37 pm] ayush pushkar     8 gb ram mobile between 10000 & 30000 --> ye v not working","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>search_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>'iphone under 30k'</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>samsung phones under 5000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>earphones between 1000 to 2000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[1:37 pm] ayush pushkar     8 gb ram mobile between 10000 &amp; 30000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[1:37 pm] ayush pushkar     8 gb ram mobile between 10000 &amp; 30000 --&gt; ye v not working</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# preposition_sentences = [\n#     \"32 inch tv under 30000\",\n#     \"Air Conditioners under 5000\",\n#     \"6gb mobiles under 1000\"\n# ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quality_tags_list = []\ncategory_tags_list = []\nbrand_list = []\npreposition_list = [] \npreposition_meaning_list = []\nfor k in tqdm(prep_df[\"search_text\"]):\n    sentence_split = k.split()\n    split_len = len(sentence_split)\n    brand_str = \"\"\n    initial_key = \"\"\n    \n    for i in range(split_len):\n        if sentence_split[i] in data_checker_dict[\"\"]:\n            start_index = i\n            j = 2\n            loop_index = i + 1\n            brand_tuple_index = tuple()\n            while loop_index <= split_len and \" \".join(sentence_split[start_index:loop_index]) in data_checker_dict[\" \".join(sentence_split[start_index:(loop_index - 1)])]:\n                if \" \".join(sentence_split[start_index:loop_index]) in data_checker_set:\n                    brand_tuple_index = (start_index, loop_index) \n                j += 1\n                loop_index += 1\n            if brand_tuple_index:\n                brand_str = ' '.join(sentence_split[brand_tuple_index[0]:brand_tuple_index[1]])\n                brand_list.append(brand_str)\n                print('brand:', brand_str, 'sentence:', k)\n                sentence_split[brand_tuple_index[0]:brand_tuple_index[1]] = []\n                break\n    if not brand_str:\n        brand_list.append(\"\")\n    k = ' '.join(sentence_split)\n    quality_tags, category_tags, preposition_tags, preposition_meaning_tags = unsupervised_feature_extraction_dependency_extraction(k)\n    quality_tags_list.append(quality_tags)\n    category_tags_list.append(category_tags)\n    preposition_list.append(preposition_tags)\n    preposition_meaning_list.append(preposition_meaning_tags)","metadata":{"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"100%|██████████| 26/26 [00:00<00:00, 92.10it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"prep_df['Quality Tags'] = quality_tags_list\nprep_df['Category Tags'] = category_tags_list\nprep_df['Brand List'] = brand_list\nprep_df['Preposition List'] = preposition_list\nprep_df['Preposition Meaning List'] = preposition_meaning_list\n!mkdir ./dev_csvs\nprep_df.to_csv('./dev_csvs/quality_category_brand_preposition_extraction.csv', sep='\\t', encoding='utf-8')","metadata":{"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
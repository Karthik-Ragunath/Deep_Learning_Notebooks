{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, TimeDistributed\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku \n\n# set seeds for reproducability\nfrom numpy.random import seed\n\nimport pandas as pd\nimport numpy as np\nimport string, os ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\n!python -m spacy download en_core_web_sm\nnlp = spacy.load('en_core_web_sm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nimport time\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport re\nimport glob\nimport random\nimport seaborn as sns\nimport string\n\nfrom IPython.display import clear_output\n\n# Hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# http://www.nltk.org/howto/wordnet.html\n\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom nltk.wsd import lesk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/qa-csv/qa.csv', delimiter='\\t', encoding='utf-8', index_col=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.rename({'Query': 'sentence'}, axis='columns')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['sent_id'] = np.arange(len(train_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['sentence'] = train_data['sentence'].fillna('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_tags = []\nsentences = train_data['sentence']\nfor query in sentences:\n    print(query)\n    nlp_text = nlp(query)\n    token_list = []\n    for token in nlp_text:\n        token_list.append(token.pos_)\n    nlp_pos = \" \".join(token_list)\n    sentence_tags.append(nlp_pos)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def return_nlp_pos(query):\n    nlp_text = nlp(query)\n    token_list = []\n    for token in nlp_text:\n        token_list.append(token.pos_)\n    nlp_pos = \" \".join(token_list)\n    return nlp_pos","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_tags[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.rename({'sentence':'Query'}, axis='columns')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['sentence'] = sentence_tags","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['sentence_clean'] = '<s ' + train_data['sentence']\ntrain_data['sentence_clean'] = train_data['sentence_clean'] + ' /s>'\n\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uni","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = train_data['sentence_clean']\ntext_list = \" \".join(map(str, text))\ntext_list[0:100]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_list = pd.DataFrame({'words':text.str.split(' ', expand = True).stack().unique()})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_count_table = pd.DataFrame()\nfor n,word in enumerate(word_list['words']):\n    # Create a list of just the word we are interested in, we use regular expressions so that part of words do not count\n    # e.g. 'ear' would be counted in each appearance of the word 'year'\n    word_count = len(re.findall(' ' + word + ' ', text_list))  \n    word_count_table = word_count_table.append(pd.DataFrame({'count':word_count}, index=[n]))\n    \n    clear_output(wait=True)\n    print('Proportion of words completed:', np.round(n/len(word_list),4)*100,'%')\n\nword_list['count'] = word_count_table['count']\n# Remove the count for the start and end of sentence notation so \n# that these do not inflate the other probabilities\nword_list['count'] = np.where(word_list['words'] == '<s' , 0,\n                     np.where(word_list['words'] == '/s>', 0,\n                     word_list['count']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_list['prob'] = word_list['count']/sum(word_list['count'])\nword_list.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unigram_table = pd.DataFrame()\n\nstart_time = time.time()\n# Loop through each sentence\n# REMOVE ROW LIMIT FOR FULL RUN\nfor index in train_data[:].index:\n    data_row = train_data.iloc[index,:]\n\n    sent_probs = pd.DataFrame()\n    # Go through each word in the sentence, lookup the probability of the word and \n    # then find the mulitplicitive product of all probabilities in the sentence.\n    sentence_cleaned = data_row['sentence_clean'].split(' ')\n    sentence_cleaned = [word for word in sentence_cleaned if word not in ('<s', '/s>')]\n    for n,word in enumerate(sentence_cleaned):\n        prob = float(word_list[word_list['words']==word]['prob'])\n        sent_probs = sent_probs.append(pd.DataFrame({'prob': prob}, index = [n]))\n    unigram = sent_probs['prob'].prod(axis=0)\n    \n    # Create a list of unigram calculation for each sentence\n    unigram_table = unigram_table.append(pd.DataFrame({'unigram':unigram},index = [index]))\n    \n    clear_output(wait=True)\n    print('Proportion of sentences completed:', np.round(index/len(train_data),4)*100,'%')\n        \nend_time = time.time()\nprint('Total run time = ', np.round(end_time-start_time,2)/60, ' minutes')\n\ntrain_data['unigram'] = unigram_table['unigram']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logarithmic Method","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unigram_table_log = pd.DataFrame()\n\nstart_time_log = time.time()\n# Loop through each sentence\n# REMOVE ROW LIMIT FOR FULL RUN\nfor index in train_data[:].index:\n    data_row = train_data.iloc[index,:]\n\n    sent_probs = pd.DataFrame()\n    # Go through each word in the sentence, lookup the probability of the word and \n    # then find the mulitplicitive product of all probabilities in the sentence.\n    sentence_cleaned = data_row['sentence_clean'].split(' ')\n    sentence_cleaned = [word for word in sentence_cleaned if word not in ('<s', '/s>')]\n    for n,word in enumerate(sentence_cleaned):\n        prob = float(word_list[word_list['words']==word]['prob'])\n        log_prob = np.log10(prob)\n        sent_probs = sent_probs.append(pd.DataFrame({'log_prob':log_prob}, index = [n]))\n        \n    unigram_log = sum(sent_probs['log_prob'])\n    \n    # Create a list of unigram calculation for each sentence\n    unigram_table_log = unigram_table_log.append(pd.DataFrame({'unigram_log':unigram_log},index = [index]))\n                                         \n    clear_output(wait=True)\n    print('Proportion of sentences completed:', np.round(index/len(train_data),4)*100,'%')\n                                                                   \nend_time_log = time.time()\nprint('Total run time = ', np.round(end_time_log-start_time_log,2)/60, ' minutes')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unigram_table_log","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unigram_table","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_1 = 'red'\nword_2 = 'dress'\n\nword_1_pos = return_nlp_pos(word_1)\nword_2_pos = return_nlp_pos(word_2)\nprint('1:', word_1_pos, '2:', word_2_pos)\n\nprob_word_1 = word_list[word_list['words'] == word_1_pos]['prob'].iloc[0]\nprob_word_2 = word_list[word_list['words'] == word_2_pos]['prob'].iloc[0]\n\nunigram_prob = prob_word_1*prob_word_2\n\nprint('The unigram probability of the word \"red\" occuring given the word \"dress\" was the previous word is: ', np.round(unigram_prob,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
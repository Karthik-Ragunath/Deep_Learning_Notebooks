{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "Seq2Seq(Attention).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthik-Ragunath/Deep_Learning_Notebooks/blob/master/Seq2Seq(Attention).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlLeh7AEMuIT"
      },
      "source": [
        "# code by Tae Hwan Jung @graykode\n",
        "# Reference : https://github.com/hunkim/PyTorchZeroToAll/blob/master/14_2_seq2seq_att.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCmbKbx7T12u",
        "outputId": "aa5b0e4f-9779-41c7-bb2f-a0edfcbf7f09"
      },
      "source": [
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "\n",
        "def make_batch():\n",
        "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
        "    # returns [np.array-dim(num_words_in_sentence, n_class)] # n_class represents one hot encoding length\n",
        "    # outer list of dim 1\n",
        "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
        "    # returns [np.array-dim(num_words_in_sentence, one_hot_encoding_length)]\n",
        "    # outer list is of dimension 1\n",
        "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
        "    # returns dimension - [[word_len]]\n",
        "    # here too outer list of size 1\n",
        "\n",
        "    # make tensor\n",
        "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
        "    # creates tensor of size (1,num_token_sent_1,n_class), (1,num_token_sent_2,n_class), (1,num_token_sent_3) \n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
        "        print(\"Hidden Layer Size:\", n_hidden)\n",
        "        # Linear for attention\n",
        "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
        "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
        "\n",
        "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
        "        enc_inputs = enc_inputs.transpose(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "        dec_inputs = dec_inputs.transpose(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
        "\n",
        "        # enc_outputs : [n_step (sequence_len), batch_size, num_directions(=1) * n_hidden], matrix F\n",
        "        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden) \n",
        "\n",
        "        trained_attn = []\n",
        "        hidden = enc_hidden\n",
        "        n_step = len(dec_inputs) # n_step - sequence_length\n",
        "        model = torch.empty([n_step, 1, n_class]) # same torch.empty(n_step, 1, n_class) - gives tensor of shape (seq_len, 1, input_feature_size)\n",
        "\n",
        "        # If there are multiple batches just make sure to add an outer for loop\n",
        "        # to traverse through different batches and to maintain 3 dimensions, use unsqueeze(batch_dim)\n",
        "        for i in range(n_step):  # each time step\n",
        "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
        "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden) # unsqueezing is done to maintain input_shape - shape maintained = (1, batch_size, n_class)\n",
        "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step] # to compute impact of rest of timestamps on one timestamp, \n",
        "                                                                                                         # thats why decoder output is 1 element (unsqueezed) and encoder output is in list \n",
        "            trained_attn.append(attn_weights.squeeze().data.numpy()) # (1,1,num_steps) gets squeezed into numpy array of size (num_steps) where num_steps = sequence_len or time_stamps\n",
        "\n",
        "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
        "            context = attn_weights.bmm(enc_outputs.transpose(0, 1)) # performing batch matrix multiplication\n",
        "            # (1,1,seq_len) * (seq_len, batch_size, n_hidden*n_directions(=1)).transpose(0,1) where n_class reders to features of input based on which one hot encoding is done\n",
        "            # (1,1,seq_len) * (batch_size, seq_len, n_hidden*n_directions(=1)) - we are considering only one batch here, therefore,\n",
        "            # (1,1,seq_len) * (1, seq_len, num_dirns(=1)*n_hidden) = (1, 1, num_dirns(=1)*n_hidden)\n",
        "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
        "            # after squeezing [n_step(=1),batch_size(=1), num_directions(=1)*n_hidden] in 0th dimension - [batch_size(=1), num_directions(=1)*n_hidden]  \n",
        "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
        "            # after squeezing [1,1, num_dirns(=1)*n_hidden] in 0th dimension - [1,  num_dirns(=1)*n_hidden]\n",
        "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
        "            # torch.cat return (1, (num_directions(=1) * n_hidden) + (num_directions(=1) * n_hidden)) = (1, 2 * num_directions(=1) * n_hidden)\n",
        "            # (1, 2 * num_directions(=1) * n_hidden) * (2*num_directions(=1)*n_hidden, n_class) # n_class represents input features used for one hot encoding\n",
        "            # which will return (1, n_class)\n",
        "\n",
        "        # make model shape [n_step, n_class]\n",
        "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
        "        # model overall shape - [seq_len, 1, n_class]\n",
        "        # [seq_len, 1, n_class].transpose(0, 1) returns shape - [1, seq_len, n_class]\n",
        "        # [seq_len, 1, n_class] squeezing on dimension 0 returns (seq_len, n_class) # seq_len is same as n_step or number_of_timestamps\n",
        "\n",
        "\n",
        "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
        "        n_step = len(enc_outputs) # n_step = seq_len\n",
        "        attn_scores = torch.zeros(n_step)  # attn_scores : [n_step]\n",
        "        # dec\n",
        "        for i in range(n_step):\n",
        "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
        "\n",
        "        # Normalize scores to weights in range 0 to 1\n",
        "        return F.softmax(attn_scores).view(1, 1, -1) # gets softmax output from list of size seq_len (time_stamps)\n",
        "        # returns dimension of size (1,1,seq_len)\n",
        "\n",
        "\n",
        "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
        "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
        "        # attn = nn.Linear(n_hidden, n_hidden) - enc_output = (batch_size, n_hidden * num_dirns) - dec_output = (Batch_size, n_hidden * num_dirns)\n",
        "        # (batch_size, n_hidden*num_dirns) * (n_hidden, n_hidden) = (batch_size, n_hidden)\n",
        "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value\n",
        "        # .view(-1) unravels the last dimension which will result in 2 dimesnional tensor becoming 1 dimensional\n",
        "        # .view(-1) will therefore return dimension - vector of dimension batch_size * n_hidden\n",
        "        # dot product will be scalar\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    n_step = 5 # number of cells(= number of Step)\n",
        "    n_hidden = 128 # number of hidden units in one cell\n",
        "\n",
        "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
        "\n",
        "    word_list = \" \".join(sentences).split()\n",
        "    word_list = list(set(word_list))\n",
        "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "    number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "\n",
        "    # print(word_list, word_dict, number_dict)\n",
        "    n_class = len(word_dict)  # vocab list\n",
        "\n",
        "    # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
        "    hidden = torch.zeros(1, 1, n_hidden)\n",
        "    # print('n_class:', n_class)\n",
        "    # print('*****************')\n",
        "    # print('hidden:', hidden)\n",
        "    # print('len hidden:', len(hidden[0][0]))\n",
        "\n",
        "    model = Attention()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    input_batch, output_batch, target_batch = make_batch()\n",
        "    # creates tensor of size (1,num_token_sent_1,n_class), (1,num_token_sent_2,n_class), (1,num_token_sent_3)\n",
        "\n",
        "    # # Train\n",
        "    # for epoch in range(2000):\n",
        "    #     optimizer.zero_grad()\n",
        "    #     output, _ = model(input_batch, hidden, output_batch)\n",
        "\n",
        "    #     loss = criterion(output, target_batch.squeeze(0))\n",
        "    #     if (epoch + 1) % 400 == 0:\n",
        "    #         print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "    #     loss.backward()\n",
        "    #     optimizer.step()\n",
        "\n",
        "    # # Test\n",
        "    # test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
        "    # test_batch = torch.FloatTensor(test_batch)\n",
        "    # predict, trained_attn = model(input_batch, hidden, test_batch)\n",
        "    # predict = predict.data.max(1, keepdim=True)[1]\n",
        "    # print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
        "\n",
        "    # # Show Attention\n",
        "    # fig = plt.figure(figsize=(5, 5))\n",
        "    # ax = fig.add_subplot(1, 1, 1)\n",
        "    # ax.matshow(trained_attn, cmap='viridis')\n",
        "    # ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
        "    # ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
        "    # plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_class: 11\n",
            "*****************\n",
            "hidden: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "len hidden: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afPYEL0mpv75"
      },
      "source": [
        "# Sample RNN\n",
        "rnn = nn.RNN(10, 20, 2)\n",
        "input = torch.randn(5, 3, 10)\n",
        "h0 = torch.randn(2, 3, 20)\n",
        "output, hn = rnn(input, h0)\n",
        "\n",
        "# rnn = nn.RNN(10, 20, 2) - # number_of_features = 10 (Used for one-hot encoding), hidden_size = 20 (output size), num_layers = 2\n",
        "# input = torch.randn(5, 3, 10) - # number_of_sequences = 5 (input_token_len [Intuitive number of cells]), number_of_batches = 3, input_size = 10 # num_features must be the same as input_size; used for one-hot encoding\n",
        "# h0 = torch.randn(2, 3, 20) - # directions * num_layers = 2, number_of_batches = 3, hidden_size = 20\n",
        "# output, hn = rnn(input, h0) - # output - (L,N,D∗Hout) - (num_sequences = 5, number_of_batches = 3, D*Hout (Dimension * Hidden_Size) = 1*20), hn - (D∗num_layers - (1*2), number_of_batches - 3, Hout (Hidden Size) - 20) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJvVBkPRp4yd",
        "outputId": "fd081349-9bc4-4b7f-b905-fda9155be74b"
      },
      "source": [
        "rnn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(10, 20, num_layers=2)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u_TaKCC5vci",
        "outputId": "3eafae07-b1b5-482e-e02c-966bedfee9eb"
      },
      "source": [
        "input"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.0831e+00,  1.1682e+00,  2.0543e+00, -1.1602e+00,  1.0265e+00,\n",
              "           7.3769e-01,  4.0249e-01, -7.4208e-01,  2.1492e-01,  7.1503e-01],\n",
              "         [ 1.0265e+00,  2.8302e+00,  9.5309e-01,  8.8380e-01, -8.7740e-01,\n",
              "           1.3232e+00,  9.6722e-02, -1.0835e+00, -1.4574e+00, -9.0026e-01],\n",
              "         [ 2.1591e-01,  1.3157e-01, -6.3713e-01,  9.0262e-01, -5.4343e-01,\n",
              "           4.0305e-01, -1.2223e-02, -6.7788e-01, -8.7928e-01,  6.3687e-01]],\n",
              "\n",
              "        [[-8.1321e-02, -4.0574e-01,  2.0943e-01, -2.0276e-01,  6.2889e-01,\n",
              "          -1.9671e+00,  1.1834e-01, -7.1259e-01, -1.7655e+00,  2.6436e-01],\n",
              "         [ 7.2928e-01,  2.1325e+00,  1.2982e+00, -3.8353e-01, -5.0894e-01,\n",
              "          -3.8734e-01,  1.5055e+00, -2.0847e-01, -1.4812e-01,  7.0768e-01],\n",
              "         [-1.0223e+00, -2.0602e+00,  2.0781e+00, -9.0671e-01, -4.0895e-02,\n",
              "          -2.2190e-01,  3.0300e-01,  9.9097e-01, -5.6988e-01,  5.8662e-02]],\n",
              "\n",
              "        [[-3.6564e-01,  1.1822e+00,  1.0823e-01, -7.5445e-01, -1.0674e+00,\n",
              "           2.3805e+00,  2.1944e-01,  4.9921e-01, -5.2055e-01, -1.2934e+00],\n",
              "         [-1.1463e-02,  5.5235e-01, -1.7442e+00,  6.0407e-01, -9.9931e-01,\n",
              "          -2.2391e-01,  2.5706e-01,  1.7325e+00, -9.0848e-02,  1.2693e-01],\n",
              "         [-2.0427e+00,  2.3673e-01,  8.4605e-01,  2.2844e-01, -1.5144e+00,\n",
              "           1.8815e+00, -1.8379e+00,  1.1027e-01,  1.4516e+00, -9.5869e-01]],\n",
              "\n",
              "        [[ 7.0426e-01,  1.9117e+00, -1.1926e+00,  8.4414e-01,  1.2979e+00,\n",
              "          -2.6090e-01,  1.6821e+00,  2.2245e-01, -1.5491e-01,  3.3697e-02],\n",
              "         [-1.2981e+00, -6.4531e-02, -9.8550e-01, -1.2634e+00,  3.2740e-01,\n",
              "           2.6296e-01, -2.0712e-02, -8.2313e-01,  5.1711e-01,  7.4775e-01],\n",
              "         [ 1.0522e+00, -2.1027e+00,  1.0612e+00, -3.6456e-01,  7.4748e-04,\n",
              "          -1.0480e+00, -1.9398e-01,  5.5248e-02, -3.7262e-01, -7.2969e-01]],\n",
              "\n",
              "        [[-5.8658e-01,  6.9012e-01,  1.5063e+00,  3.9468e-01,  5.7959e-01,\n",
              "          -1.3423e+00,  1.0849e-01,  8.0357e-01,  1.3340e+00, -8.2865e-01],\n",
              "         [-1.6288e-01,  1.1537e+00, -5.7447e-01,  5.4543e-01, -6.2336e-01,\n",
              "           1.3390e+00,  7.6050e-01, -9.3769e-01,  3.7606e-01,  3.1286e-01],\n",
              "         [ 5.6045e-01, -9.9885e-01,  1.4292e-01,  9.9423e-01,  9.6206e-01,\n",
              "          -2.6770e-01, -1.6759e+00, -1.7291e+00,  8.8400e-01,  1.8094e+00]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue9axnly5x6x",
        "outputId": "52c2614a-0ba7-4e90-8ab0-4cd223f2b68b"
      },
      "source": [
        "input.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqqO8ugn50mR",
        "outputId": "d2fc145e-7fe4-4949-b2a0-7b5f769a14a0"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGfHQcfW54HR",
        "outputId": "6700dbc2-e2a6-4ce1-ecc7-1a88f5d74b15"
      },
      "source": [
        "hn.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2yXZlbH6ShS"
      },
      "source": [
        "# Sample RNN\n",
        "rnn = nn.RNN(10, 20, 2)\n",
        "input = torch.randn(5, 3, 10)\n",
        "h0 = torch.randn(2, 3, 20)\n",
        "output, hn = rnn(input, h0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr-Gpix064t1",
        "outputId": "3fe7ddd1-149e-4146-faeb-80525f1d336d"
      },
      "source": [
        "# linear layer\n",
        "m = nn.Linear(20, 30) # 20 - size of each input sample, 30 - size of each output sample\n",
        "input = torch.randn(128, 20) # 128 - No. of input samples, 20 - size of each input sample\n",
        "output = m(input) # (128 * 20) * (20 * 30) = (128 * 30) - matrix dimension\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydQPGauRQgxW",
        "outputId": "be0d26e8-f9a3-423b-9554-98fe3ece9394"
      },
      "source": [
        "input.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmEVciPFQi1l",
        "outputId": "18171992-ff6d-4893-dd04-787a4e9afcf5"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm_xGmMLQpIt",
        "outputId": "def36d6c-d75a-42c1-d833-8b266ff4c915"
      },
      "source": [
        "# nn.functional.linear\n",
        "input_matrix = torch.randn(128, 64)\n",
        "output_matrix = torch.randn(192, 64)\n",
        "bias = torch.ones(192)\n",
        "\n",
        "# bias got broadcasted\n",
        "\n",
        "print(input_matrix.shape)\n",
        "print(output_matrix.shape)\n",
        "print(bias.shape)\n",
        "\n",
        "m = nn.functional.linear(input_matrix, output_matrix, bias=bias) # (128 * 64) * (64 * 192) + (1 * 192)\n",
        "print(m.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 64])\n",
            "torch.Size([192, 64])\n",
            "torch.Size([192])\n",
            "torch.Size([128, 192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4hHqUNuTrxe"
      },
      "source": [
        "seq_len = 1\n",
        "input_feature_size = 2\n",
        "sample_model = torch.empty([seq_len, 1, input_feature_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au8RY99NXMZE",
        "outputId": "3f84c913-0a78-4536-c1be-604cdf308bdc"
      },
      "source": [
        "type(sample_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9XCS-iMXQGy",
        "outputId": "ae2653da-7f65-4bac-bce2-ac9e03f19cdd"
      },
      "source": [
        "sample_model.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unY0eqqWXjit",
        "outputId": "db921800-9c71-47f5-f808-091bd4b1be9d"
      },
      "source": [
        "sample_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-3.1539e-22,  3.0949e-41]]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBOm-nLTXSZq"
      },
      "source": [
        "sample_model = torch.empty(seq_len, 1, input_feature_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbkYtB75Xbj5",
        "outputId": "180bb55e-3a9e-4814-dcc3-8c5f3cbd1bda"
      },
      "source": [
        "sample_model.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9o8ME7eXech",
        "outputId": "0b4f7b30-352b-40b7-ab26-3950f613659c"
      },
      "source": [
        "sample_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-3.1539e-22,  3.0949e-41]]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8bJojtsXpZR",
        "outputId": "882c9f47-d095-43cd-9e9b-45ac87d9db18"
      },
      "source": [
        "seq_len = 120\n",
        "batches = 3\n",
        "num_input_features = 5 # vocab size for one hot encoding\n",
        "seq_index = 10\n",
        "enc_inputs = torch.randn((120, 3, 5))\n",
        "enc_inputs[seq_index].unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.8313,  2.0836, -0.6478,  1.7512, -1.6786],\n",
              "         [ 0.4946, -0.2987, -0.3469,  1.8557, -0.4541],\n",
              "         [ 0.4525,  1.2246, -0.3828,  0.4145, -0.9055]]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt8P9IoBb3ed",
        "outputId": "33d2a04c-c0de-405c-8937-ea04dc4aae92"
      },
      "source": [
        "(enc_inputs[seq_index].squeeze(0)).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ7ZEECycMVp",
        "outputId": "f332fbba-309c-4521-a8a1-bec55c76bf1e"
      },
      "source": [
        "enc_inputs[seq_index].squeeze(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8313,  2.0836, -0.6478,  1.7512, -1.6786],\n",
              "        [ 0.4946, -0.2987, -0.3469,  1.8557, -0.4541],\n",
              "        [ 0.4525,  1.2246, -0.3828,  0.4145, -0.9055]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeEN4zBDcXbA",
        "outputId": "651f5a0b-48ee-4bf4-b1c8-811aaa6f7bda"
      },
      "source": [
        "enc_inputs[seq_index].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqV9fZzOci2A",
        "outputId": "1e3c8f0d-7a4d-449c-93a0-7ab7da6a7550"
      },
      "source": [
        "# Performs batch matrix multiplication\n",
        "input = torch.randn(10, 3, 4)\n",
        "mat2 = torch.randn(10, 4, 5)\n",
        "res = torch.bmm(input, mat2)\n",
        "res.size()\n",
        "#torch.Size([10, 3, 5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c00V18FdkcTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f7ec43-0f27-40b4-b040-cae66d9639c5"
      },
      "source": [
        "input_1 = torch.randn(2,3)\n",
        "input_2 = torch.randn(2,3)\n",
        "view_1 = input_1.view(-1)\n",
        "view_2 = input_2.view(-1)\n",
        "print(view_1, view_2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.1577, -0.2166,  0.0382, -0.7249,  0.1684,  1.4514]) tensor([ 0.2352, -2.8680,  0.3145, -1.0206, -0.0357,  0.4765])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDfdyWFZMogs",
        "outputId": "89a74a02-ebdb-4280-808e-fad558cbd579"
      },
      "source": [
        "dot_product = torch.dot(view_1, view_2)\n",
        "dot_product"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.0959)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4f7qAW4NB66",
        "outputId": "0e48f084-3ebc-4020-c53b-0c5321325bb6"
      },
      "source": [
        "0.1577 * 0.2352 + -0.2166 * -2.8680 + 0.0382 * 0.3145 + -0.7249 * -1.0206 + 0.1684 * -0.0357 + 1.4514 * 0.4765"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0957269"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydNQRYM5NgVq",
        "outputId": "da9e2d54-5d39-45c4-f08a-28398b4e8966"
      },
      "source": [
        "softmax = torch.nn.functional.softmax(view_1)\n",
        "softmax"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1308, 0.0900, 0.1161, 0.0541, 0.1322, 0.4769])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAPb0gcBO_Dr"
      },
      "source": [
        "softmax_dimension_changed = softmax.view(1, 1, -1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SjOcUqxPV4q",
        "outputId": "bec9c169-77c8-45b1-cb86-03218b346b08"
      },
      "source": [
        "softmax_dimension_changed"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.1308, 0.0900, 0.1161, 0.0541, 0.1322, 0.4769]]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHnAcPjZPYtZ"
      },
      "source": [
        "torch_softmax = torch.randn(1,1,12)\n",
        "squeezed = torch_softmax.squeeze().data.numpy()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnL6Zzd_SSG9",
        "outputId": "86defeef-e48b-4da6-ebac-f4a2c4f782e9"
      },
      "source": [
        "squeezed"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.7400451 , -0.55089295,  0.46025315,  0.9144252 , -0.09785257,\n",
              "        0.344277  , -0.80282664,  0.05373623,  0.32432762, -0.1767354 ,\n",
              "        0.47543785,  1.2438958 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGQBFoUKSTMz",
        "outputId": "440dc75b-0957-43fa-9d53-3e19420fcf5c"
      },
      "source": [
        "three_d_data = torch.randn(3,4,5)\n",
        "three_d_transpose_1 = three_d_data.transpose(0,1)\n",
        "print(three_d_transpose_1.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "detDmKXtXM7q",
        "outputId": "86ba4940-31c4-4229-8d46-94632cecd8d0"
      },
      "source": [
        "three_d_data_1 = torch.randn(1, 3, 4)\n",
        "three_d_data_2 = torch.randn(1, 4, 5)\n",
        "batch_multiplication = three_d_data_1.bmm(three_d_data_2)\n",
        "batch_multiplication.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khx6VBr2b8XM",
        "outputId": "bc760606-bb66-474a-ed60-009cd7b73a8f"
      },
      "source": [
        "data_1 = torch.randn(1,3)\n",
        "data_2 = torch.randn(1, 6)\n",
        "torch_concat = torch.cat((data_1, data_2), 1)\n",
        "torch_concat"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3439, -0.0776, -2.2502,  0.0083, -0.2465,  0.1979, -0.1296,  0.8472,\n",
              "         -1.5963]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTJUdlBEl01_"
      },
      "source": [
        "one_hot_feature_len = 12\n",
        "list_1 = [1,2,3,5,6]\n",
        "one_hot_feature_vecs = [np.eye(one_hot_feature_len)[list_1]]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6JSpZ_Rsmr3",
        "outputId": "bbdc1350-853d-43a6-cefa-9106e9342041"
      },
      "source": [
        "one_hot_feature_vecs"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvBr9drXs6ex",
        "outputId": "acb67d0f-8753-48d3-a88a-5f3413ed567d"
      },
      "source": [
        "one_hot_feature_vecs[0][1]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEh5h9qCtABC",
        "outputId": "8417ccce-5c9e-4144-b89c-3931b8778b23"
      },
      "source": [
        "len(one_hot_feature_vecs)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7vyl1KDtHAF",
        "outputId": "cdf506bb-46c7-4a56-f5cb-23427793f15d"
      },
      "source": [
        "one_hot_feature_vecs[0].shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLnBtSuZt4IE",
        "outputId": "60687ca0-687c-43ed-dfc0-acb9809742de"
      },
      "source": [
        "data_1 = [np.random.randn(5,12)]\n",
        "data_2 = [np.random.randn(6,12)]\n",
        "data_3 = [[1,2,3,4]]\n",
        "tensor_1 = torch.FloatTensor(data_1)\n",
        "tensor_2 = torch.FloatTensor(data_2)\n",
        "tensor_3 = torch.LongTensor(data_3)\n",
        "\n",
        "print(tensor_1.shape, tensor_2.shape, tensor_3.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 12]) torch.Size([1, 6, 12]) torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6tNzC7hv6uI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}